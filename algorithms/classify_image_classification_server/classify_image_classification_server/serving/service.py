"""BentoML service definition."""

import bentoml
from loguru import logger
import base64
from io import BytesIO
from PIL import Image
import time

from .config import get_model_config
from .exceptions import ModelInferenceError
from .metrics import (
    health_check_counter,
    model_load_counter,
    prediction_counter,
    prediction_duration,
)
from .models import load_model
from .schemas import (
    PredictRequest,
    PredictResponse,
    ClassPrediction,
    ImageResult,
    PredictionMetadata,
    ErrorDetail,
)


@bentoml.service(
    name=f"classify_image_classification_service",
    traffic={"timeout": 30},
)
class MLService:
    """æœºå™¨å­¦ä¹ æ¨¡å‹æœåŠ¡."""

    @bentoml.on_deployment
    def setup() -> None:
        """
        éƒ¨ç½²æ—¶æ‰§è¡Œä¸€æ¬¡çš„å…¨å±€åˆå§‹åŒ–.

        ç”¨äºé¢„çƒ­ç¼“å­˜ã€ä¸‹è½½èµ„æºç­‰å…¨å±€æ“ä½œ.
        ä¸æ¥æ”¶ self å‚æ•°,ç±»ä¼¼é™æ€æ–¹æ³•.
        """
        logger.info("=== Deployment setup started ===")
        # å¯ä»¥åœ¨è¿™é‡Œåšå…¨å±€åˆå§‹åŒ–,ä¾‹å¦‚:
        # - é¢„çƒ­æ¨¡å‹ç¼“å­˜
        # - ä¸‹è½½å…±äº«èµ„æº
        # - åˆå§‹åŒ–å…¨å±€è¿æ¥æ± 
        logger.info("=== Deployment setup completed ===")

    def __init__(self) -> None:
        """åˆå§‹åŒ–æœåŠ¡,åŠ è½½é…ç½®å’Œæ¨¡å‹."""
        logger.info("Service instance initializing...")
        self.config = get_model_config()
        logger.info(f"Config loaded: {self.config}")

        try:
            self.model = load_model(self.config)
            model_load_counter.labels(source=self.config.source, status="success").inc()
            logger.info("Model loaded successfully")
        except Exception as e:
            model_load_counter.labels(source=self.config.source, status="failure").inc()
            logger.error(f"Failed to load model: {e}")
            raise

    @bentoml.on_shutdown
    def cleanup(self) -> None:
        """
        æœåŠ¡å…³é—­æ—¶çš„æ¸…ç†æ“ä½œ.

        ç”¨äºé‡Šæ”¾èµ„æºã€å…³é—­è¿æ¥ç­‰.
        """
        logger.info("=== Service shutdown: cleaning up resources ===")
        # æ¸…ç†é€»è¾‘,ä¾‹å¦‚:
        # - å…³é—­æ•°æ®åº“è¿æ¥
        # - ä¿å­˜ç¼“å­˜çŠ¶æ€
        # - é‡Šæ”¾ GPU æ˜¾å­˜
        logger.info("=== Cleanup completed ===")

    def _decode_base64_image(self, img_data: str) -> Image.Image:
        """
        è§£ç base64å›¾ç‰‡ï¼Œæ”¯æŒçº¯base64å’ŒData URIæ ¼å¼.

        Args:
            img_data: Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®

        Returns:
            PIL Imageå¯¹è±¡

        Raises:
            ValueError: å›¾ç‰‡æ ¼å¼æˆ–å°ºå¯¸ä¸åˆæ³•
        """
        # ç§»é™¤Data URIå‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if img_data.startswith("data:"):
            # æå–base64éƒ¨åˆ†ï¼šdata:image/jpeg;base64,xxxxx -> xxxxx
            img_data = img_data.split(",", 1)[1]

        # è§£ç base64
        image_bytes = base64.b64decode(img_data)

        # åŠ è½½PILå›¾ç‰‡
        image = Image.open(BytesIO(image_bytes))

        # éªŒè¯æ ¼å¼
        if image.format not in ["JPEG", "PNG", "BMP", "WEBP", None]:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼: {image.format}")

        # éªŒè¯å°ºå¯¸
        if max(image.size) > 4096:
            raise ValueError(f"å›¾ç‰‡è¿‡å¤§: {image.size}, æœ€å¤§4096px")

        # è½¬æ¢ä¸ºRGBï¼ˆYOLOè¦æ±‚ï¼‰
        if image.mode != "RGB":
            image = image.convert("RGB")

        return image

    @bentoml.api
    async def predict(
        self, images: list, config: dict | None = None
    ) -> PredictResponse:
        """
        å›¾ç‰‡åˆ†ç±»é¢„æµ‹æ¥å£ï¼ˆç»Ÿä¸€æ‰¹é‡æ ¼å¼ï¼‰.

        æ”¯æŒå•å¼ å’Œæ‰¹é‡é¢„æµ‹ï¼Œè‡ªåŠ¨åˆ©ç”¨YOLOæ‰¹å¤„ç†ä¼˜åŒ–GPUåˆ©ç”¨ç‡ã€‚

        ç¤ºä¾‹ï¼š
            å•å¼ ï¼š{"images": ["base64..."], "config": {"top_k": 5}}
            æ‰¹é‡ï¼š{"images": ["img1", "img2", ...], "config": {"top_k": 3}}

        Args:
            images: Base64ç¼–ç çš„å›¾ç‰‡åˆ—è¡¨
            config: é¢„æµ‹é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            é¢„æµ‹å“åº”ï¼Œresultsä¸è¾“å…¥imagesä¸€ä¸€å¯¹åº”
        """
        from .schemas import PredictConfig

        request_start = time.time()

        # å¿«é€Ÿå¤±è´¥ï¼šå‰ç½®éªŒè¯ï¼ˆåœ¨ try å—å¤–ï¼‰
        try:
            predict_config = PredictConfig(**config) if config else PredictConfig()
            request = PredictRequest(images=images, config=predict_config)
        except Exception as e:
            logger.error(f"Request validation failed: {e}")
            return PredictResponse(
                results=[],
                metadata=PredictionMetadata(
                    model_version="unknown",
                    source=self.config.source,
                    batch_size=0,
                    total_time_ms=(time.time() - request_start) * 1000,
                    decode_time_ms=0.0,
                    predict_time_ms=0.0,
                    postprocess_time_ms=0.0,
                    avg_time_per_image_ms=0.0,
                    success_count=0,
                    failure_count=0,
                    success_rate=0.0,
                ),
                success=False,
                error=ErrorDetail(
                    code="E1000",
                    message=f"è¯·æ±‚æ ¼å¼éªŒè¯å¤±è´¥: {str(e)}",
                    details={"error_type": type(e).__name__},
                ),
            )

        batch_size = len(request.images)

        logger.info(
            f"ğŸ“¥ Received prediction request: batch_size={batch_size}, top_k={request.config.top_k}"
        )

        # ========== é˜¶æ®µ1ï¼šæ‰¹é‡è§£ç  ==========
        decode_start = time.time()
        images = []
        decode_times = []
        decode_errors = []

        for idx, img_data in enumerate(request.images):
            img_decode_start = time.time()
            try:
                image = self._decode_base64_image(img_data)
                images.append(image)
                decode_times.append((time.time() - img_decode_start) * 1000)
                decode_errors.append(None)
                logger.debug(f"âœ… Image {idx} decoded: {image.size}, {image.mode}")

            except Exception as e:
                logger.warning(f"âš ï¸  Image {idx} decode failed: {e}")
                images.append(None)
                decode_times.append((time.time() - img_decode_start) * 1000)
                decode_errors.append(str(e))

        total_decode_time = time.time() - decode_start

        # ç»Ÿè®¡æœ‰æ•ˆå›¾ç‰‡
        valid_indices = [i for i, img in enumerate(images) if img is not None]
        valid_images = [images[i] for i in valid_indices]
        valid_count = len(valid_images)
        failure_count = batch_size - valid_count

        logger.info(
            f"ğŸ“Š Decode completed: success={valid_count}, failed={failure_count}, time={total_decode_time:.3f}s"
        )

        # å…¨éƒ¨è§£ç å¤±è´¥ï¼Œæå‰è¿”å›
        if not valid_images:
            logger.error(f"âŒ All images decode failed")
            return PredictResponse(
                results=[
                    ImageResult(
                        predictions=[],
                        success=False,
                        error=decode_errors[i],
                        decode_time_ms=decode_times[i],
                    )
                    for i in range(batch_size)
                ],
                metadata=PredictionMetadata(
                    model_version=self.config.model_path or "unknown",
                    source=self.config.source,
                    batch_size=batch_size,
                    total_time_ms=(time.time() - request_start) * 1000,
                    decode_time_ms=total_decode_time * 1000,
                    predict_time_ms=0.0,
                    postprocess_time_ms=0.0,
                    avg_time_per_image_ms=0.0,
                    success_count=0,
                    failure_count=batch_size,
                    success_rate=0.0,
                ),
                success=False,
                error=ErrorDetail(
                    code="E1001",
                    message="æ‰€æœ‰å›¾ç‰‡è§£ç å¤±è´¥",
                    details={"errors": decode_errors},
                ),
            )

        # ========== é˜¶æ®µ2ï¼šæ‰¹é‡é¢„æµ‹ ==========
        logger.info(
            f"ğŸ¤– Starting prediction: {valid_count} valid images, model_source={self.config.source}"
        )

        predict_start = time.time()
        predictions = None
        predict_error = None

        try:
            # ç›´æ¥ä¼ å…¥PILå›¾ç‰‡åˆ—è¡¨ï¼ŒYOLOè‡ªåŠ¨æ‰¹å¤„ç†
            predictions = self.model.predict(valid_images)
            predict_time = time.time() - predict_start

            logger.info(f"âœ… Prediction completed successfully")
            logger.info(
                f"â±ï¸  Prediction time: {predict_time:.3f}s, {valid_count} images, {predict_time / valid_count:.3f}s per image"
            )

        except Exception as e:
            predict_time = time.time() - predict_start
            predict_error = str(e)

            logger.error(f"âŒ Prediction failed: {e}", exc_info=True)

            # é¢„æµ‹å¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰æœ‰æ•ˆå›¾ç‰‡ä¸ºå¤±è´¥
            return PredictResponse(
                results=[
                    ImageResult(
                        predictions=[],
                        success=False,
                        error=predict_error if i in valid_indices else decode_errors[i],
                        decode_time_ms=decode_times[i],
                    )
                    for i in range(batch_size)
                ],
                metadata=PredictionMetadata(
                    model_version=self.config.model_path or "unknown",
                    source=self.config.source,
                    batch_size=batch_size,
                    total_time_ms=(time.time() - request_start) * 1000,
                    decode_time_ms=total_decode_time * 1000,
                    predict_time_ms=predict_time * 1000,
                    postprocess_time_ms=0.0,
                    avg_time_per_image_ms=0.0,
                    success_count=0,
                    failure_count=batch_size,
                    success_rate=0.0,
                ),
                success=False,
                error=ErrorDetail(
                    code="E2001",
                    message=f"æ¨¡å‹é¢„æµ‹å¤±è´¥: {predict_error}",
                    details={"model_source": self.config.source},
                ),
            )

        # ========== é˜¶æ®µ3ï¼šåå¤„ç†å’Œç»„è£…ç»“æœ ==========
        postprocess_start = time.time()

        results = []
        pred_idx = 0

        for idx in range(batch_size):
            if decode_errors[idx]:
                # è§£ç å¤±è´¥çš„å›¾ç‰‡
                results.append(
                    ImageResult(
                        predictions=[],
                        success=False,
                        error=decode_errors[idx],
                        decode_time_ms=decode_times[idx],
                    )
                )
            else:
                # è§£ç æˆåŠŸçš„å›¾ç‰‡ï¼Œæå–é¢„æµ‹ç»“æœ
                pred = predictions[pred_idx]
                top_k_results = pred["top5"][: request.config.top_k]

                results.append(
                    ImageResult(
                        predictions=[
                            ClassPrediction(
                                class_id=r["class_id"],
                                class_name=r["class_name"],
                                confidence=r["confidence"],
                            )
                            for r in top_k_results
                        ],
                        success=True,
                        error=None,
                        decode_time_ms=decode_times[idx],
                    )
                )
                pred_idx += 1

        postprocess_time = time.time() - postprocess_start

        # ========== é˜¶æ®µ4ï¼šå…ƒæ•°æ®ç»Ÿè®¡ ==========
        total_time = time.time() - request_start
        success_count = sum(1 for r in results if r.success)
        failure_count = batch_size - success_count

        metadata = PredictionMetadata(
            model_version=self.config.model_path or "unknown",
            source=self.config.source,
            batch_size=batch_size,
            total_time_ms=total_time * 1000,
            decode_time_ms=total_decode_time * 1000,
            predict_time_ms=predict_time * 1000,
            postprocess_time_ms=postprocess_time * 1000,
            avg_time_per_image_ms=(predict_time * 1000) / valid_count
            if valid_count > 0
            else 0.0,
            success_count=success_count,
            failure_count=failure_count,
            success_rate=success_count / batch_size,
        )

        # æ›´æ–°PrometheusæŒ‡æ ‡
        if success_count > 0:
            prediction_counter.labels(
                model_source=self.config.source, status="success"
            ).inc(success_count)

        if failure_count > 0:
            prediction_counter.labels(
                model_source=self.config.source, status="failure"
            ).inc(failure_count)

        logger.info(
            f"âœ… Request completed: success={success_count}/{batch_size} ({metadata.success_rate:.1%}), total_time={total_time:.3f}s"
        )

        return PredictResponse(
            results=results, metadata=metadata, success=(success_count > 0), error=None
        )

    @bentoml.api
    async def health(self) -> dict:
        """å¥åº·æ£€æŸ¥æ¥å£."""
        health_check_counter.inc()
        return {
            "status": "healthy",
            "model_source": self.config.source,
            "model_version": getattr(self.model, "version", "unknown"),
        }
