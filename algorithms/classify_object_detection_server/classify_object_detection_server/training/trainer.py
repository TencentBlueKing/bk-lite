"""é€šç”¨ç›®æ ‡æ£€æµ‹è®­ç»ƒå™¨."""

from typing import Dict, Any, Optional
import mlflow
from loguru import logger
import torch

from .config.loader import TrainingConfig
from .models.base import ModelRegistry
from .preprocessing import ImagePreprocessor, ImageFeatureEngineer
from .data_loader import load_yolo_dataset
from .mlflow_utils import MLFlowUtils


class UniversalTrainer:
    """é€šç”¨ç›®æ ‡æ£€æµ‹è®­ç»ƒå™¨.

    å®ç°æ ‡å‡†çš„10æ­¥è®­ç»ƒæµç¨‹ï¼š
    1. MLflowå®éªŒè®¾ç½®
    2. æ•°æ®åŠ è½½
    3. æ•°æ®é¢„å¤„ç†
    4. æ¨¡å‹å®ä¾‹åŒ–
    5. å¼€å§‹MLflow run
    6. è®°å½•é…ç½®å‚æ•°
    7. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
    8. æ¨¡å‹è®­ç»ƒ
    9. æ¨¡å‹è¯„ä¼°
    10. æ¨¡å‹ä¿å­˜å’Œæ³¨å†Œ
    """

    def __init__(self, config: TrainingConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨.

        Args:
            config: è®­ç»ƒé…ç½®å¯¹è±¡
        """
        self.config = config
        self.model = None
        self.preprocessor = None
        self.feature_engineer = None
        self.dataset_yaml = None

        # è§£æè®¾å¤‡é…ç½®
        self.device = self._resolve_device()

        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ– - æ¨¡å‹ç±»å‹: {config.model_type}")
        logger.info(f"è®­ç»ƒè®¾å¤‡: {self.device}")

    def train(self, dataset_yaml_path: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹.

        Args:
            dataset_yaml_path: YOLOæ ¼å¼æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆdata.yaml æˆ– dataset.yamlï¼‰

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹è®­ç»ƒ - æ¨¡å‹: {self.config.model_type}")
        logger.info("=" * 60)

        # 1. è®¾ç½®MLflow
        self._setup_mlflow()

        # 2. åŠ è½½æ•°æ®
        self.dataset_yaml = self._load_data(dataset_yaml_path)

        # 3. æ•°æ®é¢„å¤„ç†
        self._preprocess_data(dataset_yaml_path)

        # 4. åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = self._create_model()

        # 5. å¼€å§‹MLflow run
        with mlflow.start_run(run_name=self.config.mlflow_run_name) as run:
            try:
                # 6. è®°å½•é…ç½®
                self._log_config()

                # 7. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
                best_params = None
                max_evals = self.config.get("hyperparams", "max_evals", default=0)
                if max_evals > 0:
                    logger.info(f"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œæœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evals}")
                    best_params = self._optimize_hyperparams()
                    if best_params:
                        MLFlowUtils.log_params_batch(best_params)
                        logger.info(f"æœ€ä¼˜è¶…å‚æ•°: {best_params}")

                # 8. è®­ç»ƒæ¨¡å‹
                self._train_model()

                # 9. è¯„ä¼°æ¨¡å‹
                test_metrics = self._evaluate_model_on_test()

                # 10. ä¿å­˜æ¨¡å‹åˆ°MLflowå¹¶æ³¨å†Œ
                model_uri = self._save_model_to_mlflow()
                self._register_model(model_uri)

                logger.info("=" * 60)
                logger.info("è®­ç»ƒå®Œæˆï¼")
                logger.info("=" * 60)

                return {
                    "model": self.model,
                    "test_metrics": test_metrics,
                    "run_id": run.info.run_id,
                    "best_params": best_params,
                }

            except Exception as e:
                logger.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
                raise

    def _resolve_device(self) -> str:
        """
        è§£æè®¾å¤‡é…ç½®.

        Returns:
            YOLO deviceå‚æ•°ï¼ˆå¦‚'0', 'cpu', '0,1,2,3'ï¼‰
        """
        device_mode = self.config.get_device_config()

        if device_mode == "cpu":
            return self._resolve_cpu()
        elif device_mode == "gpu":
            return self._resolve_gpu()
        elif device_mode == "gpus":
            return self._resolve_gpus()
        elif device_mode == "auto":
            return self._resolve_auto()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„deviceæ¨¡å¼: {device_mode}")

    def _resolve_cpu(self) -> str:
        """å¼ºåˆ¶CPUæ¨¡å¼."""
        if torch.cuda.is_available():
            logger.warning("âš ï¸  é…ç½®æŒ‡å®šä½¿ç”¨CPUï¼Œä½†æ£€æµ‹åˆ°å¯ç”¨GPU")
        logger.info("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ")
        return "cpu"

    def _resolve_gpu(self) -> str:
        """å•GPUæ¨¡å¼."""
        if not torch.cuda.is_available():
            raise RuntimeError(
                "é…ç½®æŒ‡å®šä½¿ç”¨GPUï¼Œä½†æœªæ£€æµ‹åˆ°å¯ç”¨GPUã€‚\n"
                "è¯·æ£€æŸ¥ï¼š\n"
                "1. æ˜¯å¦å®‰è£…äº†GPUç‰ˆæœ¬çš„PyTorch\n"
                "2. CUDAæ˜¯å¦æ­£ç¡®å®‰è£…\n"
                "3. æˆ–å°†é…ç½®æ”¹ä¸º device='cpu'"
            )

        gpu_name = torch.cuda.get_device_name(0)
        logger.info(f"ğŸš€ ä½¿ç”¨å•GPUè®­ç»ƒ: {gpu_name} (GPU 0)")
        return "0"

    def _resolve_gpus(self) -> str:
        """å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼."""
        if not torch.cuda.is_available():
            raise RuntimeError("é…ç½®æŒ‡å®šå¤šGPUè®­ç»ƒï¼Œä½†æœªæ£€æµ‹åˆ°å¯ç”¨GPU")

        available_gpus = torch.cuda.device_count()

        if available_gpus < 2:
            logger.warning(
                f"âš ï¸  é…ç½®ä¸ºå¤šGPUæ¨¡å¼ï¼Œä½†åªæœ‰{available_gpus}å—GPU\n"
                f"   è‡ªåŠ¨é™çº§ä¸ºå•GPUæ¨¡å¼"
            )
            return "0"

        # ä½¿ç”¨æ‰€æœ‰GPU
        gpu_ids = list(range(available_gpus))
        gpu_names = [torch.cuda.get_device_name(i) for i in gpu_ids]

        logger.info(
            f"ğŸš€ğŸš€ ä½¿ç”¨å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ ({len(gpu_ids)}å—GPU):\n"
            + "\n".join([f"   GPU {i}: {name}" for i, name in zip(gpu_ids, gpu_names)])
        )

        # YOLOçš„å¤šGPUæ ¼å¼ï¼š'0,1,2,3'
        yolo_device = ",".join(map(str, gpu_ids))
        return yolo_device

    def _resolve_auto(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å¼."""
        if not torch.cuda.is_available():
            logger.warning(
                "âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ\n   æç¤ºï¼šCPUè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨GPU"
            )
            return "cpu"

        gpu_count = torch.cuda.device_count()
        gpu_names = [torch.cuda.get_device_name(i) for i in range(gpu_count)]

        if gpu_count == 1:
            logger.info(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ï¼šä½¿ç”¨å•GPUè®­ç»ƒ\n   GPU 0: {gpu_names[0]}")
            return "0"
        else:
            logger.info(
                f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ï¼šå‘ç°{gpu_count}å—GPUï¼Œä½¿ç”¨å•GPUæ¨¡å¼ï¼ˆGPU 0ï¼‰\n"
                + "\n".join([f"   GPU {i}: {name}" for i, name in enumerate(gpu_names)])
                + f"\n\nğŸ’¡ æç¤ºï¼šå¦‚éœ€å¤šGPUè®­ç»ƒï¼Œè¯·è®¾ç½®é…ç½®ï¼š\n"
                f'   "device": "gpus"'
            )
            return "0"

    def _setup_mlflow(self):
        """è®¾ç½®MLflowå®éªŒ."""
        tracking_uri = self.config.mlflow_tracking_uri
        experiment_name = self.config.mlflow_experiment_name

        MLFlowUtils.setup_experiment(tracking_uri, experiment_name)

    def _load_data(self, dataset_yaml_path: str) -> str:
        """
        åŠ è½½YOLOæ ¼å¼æ•°æ®é›†.

        Returns:
            æ•°æ®é›†é…ç½®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        """
        logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_yaml_path}")
        return load_yolo_dataset(dataset_yaml_path)

    def _preprocess_data(self, dataset_yaml_path: str):
        """
        æ•°æ®é¢„å¤„ç†.
        """
        logger.info("æ•°æ®é¢„å¤„ç†...")

        # åˆå§‹åŒ–é¢„å¤„ç†å™¨
        self.preprocessor = ImagePreprocessor(self.config.preprocessing_config)

        # éªŒè¯æ•°æ®é›†
        validation_result = self.preprocessor.validate_dataset(dataset_yaml_path)
        if not validation_result["valid"]:
            logger.error(f"æ•°æ®é›†éªŒè¯å¤±è´¥: {validation_result['errors']}")
            raise ValueError("æ•°æ®é›†éªŒè¯å¤±è´¥")

        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        self.feature_engineer = ImageFeatureEngineer(
            self.config.feature_engineering_config
        )

        logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")

    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹å®ä¾‹."""
        model_type = self.config.model_type
        hyperparams = self.config.hyperparams_config.copy()

        # å°†è®¾å¤‡ä¿¡æ¯ä¼ é€’ç»™æ¨¡å‹ï¼ˆä¾›è¶…å‚æ•°ä¼˜åŒ–ä½¿ç”¨ï¼‰
        hyperparams["_device"] = self.device

        logger.info(f"åˆ›å»ºæ¨¡å‹: {model_type}")
        model = ModelRegistry.create(model_type, **hyperparams)

        return model

    def _log_config(self):
        """è®°å½•é…ç½®åˆ°MLflow."""
        logger.info("è®°å½•é…ç½®åˆ°MLflow...")

        # ä½¿ç”¨ç»Ÿä¸€çš„to_dict()æ–¹æ³•å¯¼å‡ºé…ç½®
        config_dict = self.config.to_dict()

        # å±•å¼€åµŒå¥—çš„é…ç½®
        flat_config = {}
        for key, value in config_dict.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    flat_config[f"{key}.{sub_key}"] = sub_value
            else:
                flat_config[key] = value

        # è®°å½•è®¾å¤‡ä¿¡æ¯
        flat_config["device"] = self.device

        MLFlowUtils.log_params_batch(flat_config)
        logger.debug(f"é…ç½®å·²è®°å½•åˆ° MLflow")

    def _optimize_hyperparams(self) -> Optional[Dict[str, Any]]:
        """è¶…å‚æ•°ä¼˜åŒ–."""
        max_evals = self.config.get("hyperparams", "max_evals", default=0)
        if max_evals == 0:
            return None

        try:
            return self.model.optimize_hyperparams(
                self.dataset_yaml, self.dataset_yaml, max_evals
            )
        except Exception as e:
            logger.error(f"è¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return None

    def _train_model(self):
        """è®­ç»ƒæ¨¡å‹."""
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # å°†feature_engineeringé…ç½®ä¼ é€’ç»™æ¨¡å‹ï¼ˆç”¨äºYOLOæ•°æ®å¢å¼ºï¼‰
        feature_config = self.config.feature_engineering_config
        if feature_config:
            augment_enabled = feature_config.get("enabled", True)
            if not augment_enabled:
                self.model.hyperparams["augment"] = False
                logger.info("âš ï¸  æ•°æ®å¢å¼ºå·²ç¦ç”¨")
            else:
                # ä¼ é€’æ£€æµ‹ä¸“ç”¨å¢å¼ºå‚æ•°ç»™YOLO
                augment_params = {}

                # å‡ ä½•å˜æ¢å‚æ•°
                for key in [
                    "degrees",
                    "translate",
                    "scale",
                    "shear",
                    "perspective",
                    "fliplr",
                    "flipud",
                ]:
                    if key in feature_config:
                        augment_params[key] = feature_config[key]

                # é¢œè‰²å¢å¼ºå‚æ•°
                for key in ["hsv_h", "hsv_s", "hsv_v"]:
                    if key in feature_config:
                        augment_params[key] = feature_config[key]

                # æ£€æµ‹ä¸“ç”¨é«˜çº§å¢å¼º
                for key in ["mosaic", "mixup", "copy_paste"]:
                    if key in feature_config:
                        augment_params[key] = feature_config[key]

                if augment_params:
                    self.model.hyperparams.update(augment_params)
                    logger.info(f"âœ“ è‡ªå®šä¹‰æ•°æ®å¢å¼ºå‚æ•°: {augment_params}")

        self.model.fit(self.dataset_yaml, device=self.device)
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")

    def _evaluate_model_on_test(self) -> Dict[str, float]:
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹."""
        logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
        test_metrics = self.model.evaluate(
            self.dataset_yaml, prefix="test", log_artifacts=True
        )

        # è®°å½•æŒ‡æ ‡åˆ°MLflow
        MLFlowUtils.log_metrics_batch(test_metrics, prefix="")

        # è¾“å‡ºåˆ°æ—¥å¿—ï¼ˆåªè¾“å‡ºæ•°å€¼æŒ‡æ ‡ï¼‰
        summary = {
            k: v
            for k, v in test_metrics.items()
            if not k.startswith("_") and isinstance(v, (int, float))
        }
        logger.info(f"æµ‹è¯•é›†è¯„ä¼°å®Œæˆ: {summary}")

        return test_metrics

    def _save_model_to_mlflow(self) -> str:
        """ä¿å­˜æ¨¡å‹åˆ°MLflow."""
        logger.info("ä¿å­˜æ¨¡å‹åˆ°MLflow...")

        try:
            self.model.save_mlflow(artifact_path="model")
            logger.info("æ¨¡å‹å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise

        model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
        return model_uri

    def _register_model(self, model_uri: str):
        """æ³¨å†Œæ¨¡å‹åˆ°MLflow Model Registry."""
        if not model_uri:
            logger.warning("æ¨¡å‹URIä¸ºç©ºï¼Œè·³è¿‡æ³¨å†Œ")
            return

        model_name = self.config.model_name

        try:
            logger.info(f"æ³¨å†Œæ¨¡å‹åˆ°Model Registry: {model_name}")
            model_version = mlflow.register_model(model_uri, model_name)
            logger.info(f"âœ“ æ¨¡å‹æ³¨å†ŒæˆåŠŸ: {model_name}, ç‰ˆæœ¬: {model_version.version}")
        except Exception as e:
            logger.warning(f"æ¨¡å‹æ³¨å†Œå¤±è´¥: {e}")
