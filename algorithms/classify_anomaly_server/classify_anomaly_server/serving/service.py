"""BentoML service definition."""

import bentoml
from loguru import logger
import time
import os

from .config import get_model_config
from .exceptions import ModelInferenceError
from .metrics import (
    health_check_counter,
    model_load_counter,
    prediction_counter,
    prediction_duration,
)
from .models import load_model
from .schemas import PredictRequest, PredictResponse


@bentoml.service(
    name=f"classify_anomaly_service",
    traffic={"timeout": 30},
)
class MLService:
    """æœºå™¨å­¦ä¹ æ¨¡å‹æœåŠ¡."""

    @bentoml.on_deployment
    def setup() -> None:
        """
        éƒ¨ç½²æ—¶æ‰§è¡Œä¸€æ¬¡çš„å…¨å±€åˆå§‹åŒ–.

        ç”¨äºé¢„çƒ­ç¼“å­˜ã€ä¸‹è½½èµ„æºç­‰å…¨å±€æ“ä½œ.
ä¸æ¥æ”¶ self å‚æ•°,ç±»ä¼¼é™æ€æ–¹æ³•.
        """
        logger.info("=== Deployment setup started ===")
        # å¯ä»¥åœ¨è¿™é‡Œåšå…¨å±€åˆå§‹åŒ–,ä¾‹å¦‚:
        # - é¢„çƒ­æ¨¡å‹ç¼“å­˜
        # - ä¸‹è½½å…±äº«èµ„æº
        # - åˆå§‹åŒ–å…¨å±€è¿æ¥æ± 
        logger.info("=== Deployment setup completed ===")

    def __init__(self) -> None:
        """åˆå§‹åŒ–æœåŠ¡,åŠ è½½é…ç½®å’Œæ¨¡å‹."""
        logger.info("Service instance initializing...")
        self.config = get_model_config()
        logger.info(f"Config loaded: {self.config}")

        try:
            load_start = time.time()
            self.model = load_model(self.config)
            load_time = time.time() - load_start
            
            model_load_counter.labels(
                source=self.config.source, status="success").inc()
            logger.info(f"â±ï¸  Model loaded successfully in {load_time:.3f}s: {self.config.mlflow_model_uri or 'local/dummy'}")
            
        except Exception as e:
            model_load_counter.labels(
                source=self.config.source, status="failure").inc()
            logger.error(f"âŒ Failed to load model: {e}", exc_info=True)
            
            # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦å…è®¸é™çº§åˆ° DummyModel
            allow_fallback = os.getenv("ALLOW_DUMMY_FALLBACK", "false").lower() == "true"
            
            if allow_fallback:
                from .models.dummy_model import DummyModel
                logger.warning("âš ï¸  ALLOW_DUMMY_FALLBACK=true, using DummyModel as fallback")
                self.model = DummyModel()
                model_load_counter.labels(
                    source="dummy_fallback", status="success").inc()
            else:
                logger.error(
                    "Model loading failed and fallback is disabled. "
                    "Set ALLOW_DUMMY_FALLBACK=true to enable DummyModel fallback."
                )
                raise RuntimeError(
                    f"Failed to load model from source '{self.config.source}'. "
                    "Service cannot start without a valid model. "
                    "Enable fallback with ALLOW_DUMMY_FALLBACK=true for development/testing."
                ) from e

    @bentoml.on_shutdown
    def cleanup(self) -> None:
        """
        æœåŠ¡å…³é—­æ—¶çš„æ¸…ç†æ“ä½œ.

        ç”¨äºé‡Šæ”¾èµ„æºã€å…³é—­è¿æ¥ç­‰.
        """
        logger.info("=== Service shutdown: cleaning up resources ===")
        # æ¸…ç†é€»è¾‘,ä¾‹å¦‚:
        # - å…³é—­æ•°æ®åº“è¿æ¥
        # - ä¿å­˜ç¼“å­˜çŠ¶æ€
        # - é‡Šæ”¾ GPU æ˜¾å­˜
        logger.info("=== Cleanup completed ===")

    @bentoml.api
    async def predict(
        self,
        data: list,
        config: dict = None
    ) -> PredictResponse:
        """
        å¼‚å¸¸æ£€æµ‹æ¥å£.

        Args:
            data: æ—¶é—´åºåˆ—æ•°æ®ç‚¹åˆ—è¡¨
            config: æ£€æµ‹é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            å¼‚å¸¸æ£€æµ‹å“åº”
        """
        import pandas as pd
        
        request_start = time.time()
        
        # æ„é€  PredictRequest å¯¹è±¡è¿›è¡ŒéªŒè¯
        from .schemas import TimeSeriesPoint, DetectionConfig, ResponseMetadata, ErrorDetail, AnomalyPoint
        try:
            data_points = [TimeSeriesPoint(**point) for point in data]
            detect_config = DetectionConfig(**config) if config else None
            request = PredictRequest(data=data_points, config=detect_config)
        except Exception as e:
            logger.error(f"Request validation failed: {e}")
            # è¿”å›éªŒè¯å¤±è´¥å“åº”
            return PredictResponse(
                success=False,
                results=None,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri if hasattr(self.config, 'mlflow_model_uri') else None,
                    input_data_points=len(data) if data else 0,
                    detected_anomalies=0,
                    anomaly_rate=0.0,
                    input_frequency=None,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=ErrorDetail(
                    code="E1000",
                    message=f"è¯·æ±‚æ ¼å¼éªŒè¯å¤±è´¥: {str(e)}",
                    details={"error_type": type(e).__name__}
                )
            )
        
        logger.info(f"ğŸ“¥ Received anomaly detection request: data_points={len(request.data)}")

        try:
            # è½¬æ¢ä¸ºæ—¶é—´åºåˆ—
            series = request.to_series()
            
            logger.info(f"ğŸ“Š Input data range: {series.index[0]} to {series.index[-1]}")
            
            # æ¨æ–­é¢‘ç‡ï¼ˆå®½æ¾æ¨¡å¼ï¼Œå…è®¸ä¸è§„åˆ™åºåˆ—ï¼‰
            inferred_freq = None
            try:
                inferred_freq = pd.infer_freq(series.index)
                if inferred_freq:
                    logger.info(f"ğŸ•’ Detected frequency: {inferred_freq}")
            except Exception:
                logger.warning("âš ï¸  Could not infer frequency, treating as irregular time series")
            
            # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
            model_info = f"source={self.config.source}, type={type(self.model).__name__}"
            if self.config.source == "local":
                model_info += f", path={self.config.model_path}"
            elif self.config.source == "mlflow":
                model_info += f", uri={self.config.mlflow_model_uri}"
            
            logger.info(f"ğŸ¤– Model info: {model_info}")
            logger.info(f"ğŸ” Starting anomaly detection...")
            
            detect_start = time.time()
            
            # å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼ˆç»Ÿä¸€å­—å…¸æ ¼å¼ï¼‰
            model_input = {'data': series}
            if request.config and request.config.threshold is not None:
                model_input['threshold'] = request.config.threshold
            
            # è°ƒç”¨æ¨¡å‹æ£€æµ‹ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
            detection_result = self.model.predict(model_input)
            
            detect_time = time.time() - detect_start
            
            logger.info(f"âœ… Detection completed successfully")
            logger.info(f"â±ï¸  Detection time: {detect_time:.3f}s")
            
            # è§£ææ£€æµ‹ç»“æœ
            # æœŸæœ›æ ¼å¼: {'labels': [0,1,0,...], 'scores': [0.1,0.9,0.2,...], 'probabilities': [0.05,0.95,...]}
            labels = detection_result.get('labels', [])
            scores = detection_result.get('scores', [])
            probabilities = detection_result.get('probabilities', [])
            
            if len(labels) != len(request.data) or len(scores) != len(request.data):
                raise ValueError(
                    f"æ¨¡å‹è¿”å›ç»“æœé•¿åº¦ä¸åŒ¹é…: è¾“å…¥{len(request.data)}ä¸ªç‚¹, "
                    f"è¿”å›labels={len(labels)}, scores={len(scores)}"
                )
            
            # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœæ¨¡å‹æ²¡æœ‰è¿”å›probabilitiesï¼Œä½¿ç”¨scoresä½œä¸ºfallback
            if len(probabilities) != len(request.data):
                logger.warning("æ¨¡å‹æœªè¿”å›probabilitiesï¼Œä½¿ç”¨scoresä½œä¸ºfallback")
                probabilities = scores
            
            # æ„é€ ç»“æœç‚¹
            result_points = []
            anomaly_count = 0
            for i, point in enumerate(request.data):
                label = int(labels[i])  # 0=æ­£å¸¸, 1=å¼‚å¸¸
                if label == 1:
                    anomaly_count += 1
                    
                result_points.append(AnomalyPoint(
                    timestamp=point.timestamp,
                    value=point.value,
                    label=label,
                    anomaly_score=float(scores[i]),
                    anomaly_probability=float(probabilities[i])
                ))
            
            anomaly_rate = anomaly_count / len(request.data) if len(request.data) > 0 else 0.0
            
            # æ„é€ æˆåŠŸå“åº”
            response = PredictResponse(
                success=True,
                results=result_points,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri if hasattr(self.config, 'mlflow_model_uri') else None,
                    input_data_points=len(request.data),
                    detected_anomalies=anomaly_count,
                    anomaly_rate=anomaly_rate,
                    input_frequency=inferred_freq,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=None
            )
            
            total_time = time.time() - request_start
            logger.info(f"ğŸ“ˆ Detection summary: {anomaly_count}/{len(request.data)} anomalies ({anomaly_rate:.2%})")
            logger.info(f"â±ï¸  Total request time: {total_time:.3f}s")
            
            prediction_counter.labels(
                model_source=self.config.source,
                status="success",
            ).inc()
            
            return response
            
        except ValueError as e:
            # éªŒè¯é”™è¯¯
            logger.error(f"Validation error: {e}")
            prediction_counter.labels(
                model_source=self.config.source,
                status="failure",
            ).inc()
            return PredictResponse(
                success=False,
                results=None,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri if hasattr(self.config, 'mlflow_model_uri') else None,
                    input_data_points=len(data) if data else 0,
                    detected_anomalies=0,
                    anomaly_rate=0.0,
                    input_frequency=None,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=ErrorDetail(
                    code="E1001",
                    message=str(e),
                    details={"error_type": "ValidationError"}
                )
            )
            
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼ˆæ¨¡å‹æ£€æµ‹å¤±è´¥ç­‰ï¼‰
            logger.error(f"Detection failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            prediction_counter.labels(
                model_source=self.config.source,
                status="failure",
            ).inc()
            
            return PredictResponse(
                success=False,
                results=None,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri if hasattr(self.config, 'mlflow_model_uri') else None,
                    input_data_points=len(data) if data else 0,
                    detected_anomalies=0,
                    anomaly_rate=0.0,
                    input_frequency=None,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=ErrorDetail(
                    code="E2002",
                    message=f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}",
                    details={"error_type": type(e).__name__}
                )
            )

    @bentoml.api
    async def health(self) -> dict:
        """å¥åº·æ£€æŸ¥æ¥å£."""
        health_check_counter.inc()
        return {
            "status": "healthy",
            "model_source": self.config.source,
            "model_version": getattr(self.model, "version", "unknown"),
        }
