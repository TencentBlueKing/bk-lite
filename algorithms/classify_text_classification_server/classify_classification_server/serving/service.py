"""BentoML service definition."""

import time
from datetime import datetime
from typing import Optional
import numpy as np
import pandas as pd

import bentoml
from loguru import logger

from .config import get_model_config
from .exceptions import ModelInferenceError
from .metrics import (
    health_check_counter,
    model_load_counter,
    prediction_counter,
    prediction_duration,
)
from .models import load_model
from .schemas import (
    PredictRequest,
    PredictResponse,
    PredictionConfig,
    ClassificationResult,
    ClassificationLabel,
    FeatureImportance,
    TextWarning,
    PredictionSummary,
    ResponseMetadata,
    ErrorDetail,
)


# å¸¸é‡å®šä¹‰
MAX_TEXT_LENGTH = 5000
TEXT_SNIPPET_LENGTH = 100


@bentoml.service(
    name="classify_text_classification_service",
    traffic={"timeout": 60},
)
class MLService:
    """æ–‡æœ¬åˆ†ç±»æ¨¡å‹æœåŠ¡."""

    @bentoml.on_deployment
    def setup() -> None:
        """
        éƒ¨ç½²æ—¶æ‰§è¡Œä¸€æ¬¡çš„å…¨å±€åˆå§‹åŒ–.

        ç”¨äºé¢„çƒ­ç¼“å­˜ã€ä¸‹è½½èµ„æºç­‰å…¨å±€æ“ä½œ.
        ä¸æ¥æ”¶ self å‚æ•°,ç±»ä¼¼é™æ€æ–¹æ³•.
        """
        logger.info("=== Deployment setup started ===")
        logger.info("=== Deployment setup completed ===")

    def __init__(self) -> None:
        """åˆå§‹åŒ–æœåŠ¡,åŠ è½½é…ç½®å’Œæ¨¡å‹."""
        logger.info("Service instance initializing...")
        self.config = get_model_config()
        logger.info(f"Config loaded: {self.config}")

        try:
            load_start = time.time()
            self.model = load_model(self.config)
            load_time = time.time() - load_start

            model_load_counter.labels(source=self.config.source, status="success").inc()
            logger.info(f"â±ï¸  Model loaded successfully in {load_time:.3f}s")

        except Exception as e:
            model_load_counter.labels(source=self.config.source, status="failure").inc()
            logger.error(f"âŒ Failed to load model: {e}", exc_info=True)
            raise RuntimeError(
                f"Failed to load model from source '{self.config.source}'. "
                "Service cannot start without a valid model."
            ) from e

    @bentoml.on_shutdown
    def cleanup(self) -> None:
        """
        æœåŠ¡å…³é—­æ—¶çš„æ¸…ç†æ“ä½œ.

        ç”¨äºé‡Šæ”¾èµ„æºã€å…³é—­è¿æ¥ç­‰.
        """
        logger.info("=== Service shutdown: cleaning up resources ===")
        logger.info("=== Cleanup completed ===")

    @bentoml.api
    async def predict(self, texts: list[str], config: dict = None) -> PredictResponse:
        """
        æ–‡æœ¬åˆ†ç±»é¢„æµ‹æ¥å£ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰.

        Args:
            texts: å¾…åˆ†ç±»çš„æ–‡æœ¬åˆ—è¡¨ï¼ˆ1-1000æ¡ï¼‰
            config: é¢„æµ‹é…ç½®ï¼ˆå¯é€‰ï¼‰
                - top_k: è¿”å›Top-Kç»“æœï¼Œé»˜è®¤3
                - return_probabilities: æ˜¯å¦è¿”å›æ‰€æœ‰ç±»åˆ«æ¦‚ç‡ï¼Œé»˜è®¤True
                - return_feature_importance: æ˜¯å¦è¿”å›ç‰¹å¾é‡è¦æ€§ï¼Œé»˜è®¤True
                - max_features: è¿”å›æœ€å¤šNä¸ªé‡è¦ç‰¹å¾ï¼Œé»˜è®¤10

        Returns:
            ç»“æ„åŒ–é¢„æµ‹å“åº”ï¼ŒåŒ…å«resultsã€summaryã€metadata
        """
        request_start = time.time()
        request_time = datetime.utcnow().isoformat() + "Z"

        # å¿«é€Ÿå¤±è´¥ï¼šå‰ç½®éªŒè¯ï¼ˆåœ¨ try å—å¤–ï¼‰
        try:
            # æ„é€ é…ç½®å¯¹è±¡
            pred_config = PredictionConfig(**config) if config else PredictionConfig()

            # æ„é€ è¯·æ±‚å¯¹è±¡è¿›è¡ŒéªŒè¯
            request = PredictRequest(texts=texts, config=pred_config)

        except Exception as e:
            logger.error(f"Request validation failed: {e}")
            return self._create_error_response(
                code="E1000",
                message=f"è¯·æ±‚éªŒè¯å¤±è´¥: {str(e)}",
                details={
                    "error_type": type(e).__name__,
                    "input_size": len(texts) if texts else 0,
                },
                request_time=request_time,
                execution_time_ms=(time.time() - request_start) * 1000,
            )

        logger.info(f"ğŸ“¥ Received classification request: {len(request.texts)} texts")

        # 2. æ–‡æœ¬é¢„å¤„ç†ï¼ˆæˆªæ–­å¤„ç†ï¼‰
        processed_texts, text_warnings = self._preprocess_texts(request.texts)
        logger.debug(f"Preprocessed texts: {processed_texts}")
        logger.debug(
            f"Processed texts type: {type(processed_texts)}, length: {len(processed_texts) if processed_texts else 0}"
        )

        try:
            # 3. æ‰¹é‡æ¨ç†
            with prediction_duration.labels(model_source=self.config.source).time():
                predict_start = time.time()

                # è°ƒç”¨æ¨¡å‹é¢„æµ‹ï¼ˆMLflow PyFuncæ ‡å‡†æ¥å£ï¼‰
                # MLflowåŠ è½½åçš„æ¨¡å‹ä½¿ç”¨æ ‡å‡†æ¥å£ï¼špredict(data)
                # MLflowå†…éƒ¨ä¼šè‡ªåŠ¨å°†dataä¼ é€’ç»™è‡ªå®šä¹‰åŒ…è£…å™¨çš„model_inputå‚æ•°
                logger.debug(f"Calling model.predict with texts: {processed_texts}")
                model_output = self.model.predict(processed_texts)

                predict_time = (time.time() - predict_start) * 1000
                logger.info(f"â±ï¸  Model prediction completed in {predict_time:.1f}ms")

            # 4. è§£ææ¨¡å‹è¾“å‡ºå¹¶æ„é€ ç»“æœ
            results = self._build_results(
                original_texts=request.texts,
                processed_texts=processed_texts,
                model_output=model_output,
                text_warnings=text_warnings,
                config=request.config,
            )

            # 5. è®¡ç®—æ±‡æ€»ç»Ÿè®¡
            summary = self._compute_summary(
                results=results, processing_time_ms=(time.time() - request_start) * 1000
            )

            # 6. æ„é€ å…ƒæ•°æ®
            metadata = self._build_metadata(
                config=request.config,
                request_time=request_time,
                execution_time_ms=(time.time() - request_start) * 1000,
            )

            # 7. è®°å½•æŒ‡æ ‡
            prediction_counter.labels(
                model_source=self.config.source, status="success"
            ).inc()

            logger.info(
                f"âœ… Classification completed: {summary.total_samples} texts, "
                f"avg_probability={summary.avg_probability:.4f}, "
                f"time={summary.processing_time_ms:.1f}ms"
            )

            return PredictResponse(
                success=True,
                results=results,
                summary=summary,
                metadata=metadata,
                error=None,
            )

        except Exception as e:
            logger.error(f"âŒ Prediction failed: {e}", exc_info=True)

            prediction_counter.labels(
                model_source=self.config.source, status="failure"
            ).inc()

            return self._create_error_response(
                code="E2001",
                message=f"æ¨¡å‹æ¨ç†å¤±è´¥: {str(e)}",
                details={
                    "error_type": type(e).__name__,
                    "model_source": self.config.source,
                },
                request_time=request_time,
                execution_time_ms=(time.time() - request_start) * 1000,
            )

    def _preprocess_texts(
        self, texts: list[str]
    ) -> tuple[list[str], list[list[TextWarning]]]:
        """
        é¢„å¤„ç†æ–‡æœ¬ï¼ˆæˆªæ–­è¶…é•¿æ–‡æœ¬ï¼‰.

        Args:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨

        Returns:
            (å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨, æ¯æ¡æ–‡æœ¬çš„è­¦å‘Šåˆ—è¡¨)
        """
        processed_texts = []
        all_warnings = []

        for text in texts:
            warnings = []
            processed_text = text

            # æ£€æŸ¥å¹¶æˆªæ–­è¶…é•¿æ–‡æœ¬
            if len(text) > MAX_TEXT_LENGTH:
                processed_text = text[:MAX_TEXT_LENGTH]
                warnings.append(
                    TextWarning(
                        type="TEXT_TRUNCATED",
                        message=f"æ–‡æœ¬è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶ï¼ˆ{MAX_TEXT_LENGTH}å­—ç¬¦ï¼‰ï¼Œå·²è‡ªåŠ¨æˆªæ–­",
                        original_length=len(text),
                        truncated_length=MAX_TEXT_LENGTH,
                    )
                )
                logger.warning(
                    f"Text truncated: {len(text)} -> {MAX_TEXT_LENGTH} chars"
                )

            processed_texts.append(processed_text)
            all_warnings.append(warnings)

        return processed_texts, all_warnings

    def _build_results(
        self,
        original_texts: list[str],
        processed_texts: list[str],
        model_output: pd.DataFrame,
        text_warnings: list[list[TextWarning]],
        config: PredictionConfig,
    ) -> list[ClassificationResult]:
        """
        æ„é€ åˆ†ç±»ç»“æœåˆ—è¡¨.

        Args:
            original_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            processed_texts: å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
            model_output: æ¨¡å‹è¾“å‡ºDataFrame
            text_warnings: æ–‡æœ¬è­¦å‘Šåˆ—è¡¨
            config: é¢„æµ‹é…ç½®

        Returns:
            åˆ†ç±»ç»“æœåˆ—è¡¨
        """
        results = []

        for i, (original_text, processed_text) in enumerate(
            zip(original_texts, processed_texts)
        ):
            # æå–å½“å‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
            row = model_output.iloc[i]

            prediction = row["prediction"]
            probability = float(row["probability"])

            # æå–æ‰€æœ‰ç±»åˆ«æ¦‚ç‡
            prob_columns = [
                col for col in model_output.columns if col.startswith("prob_")
            ]
            all_probs = {
                col.replace("prob_", ""): float(row[col]) for col in prob_columns
            }

            # æ„é€ Top-Kç»“æœ
            top_predictions = self._get_top_k_predictions(all_probs, config.top_k)

            # æ¦‚ç‡ä¿ç•™4ä½å°æ•°
            probability = round(float(row["probability"]), 4)

            # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰
            feature_importance = None
            if config.return_feature_importance:
                feature_importance = self._get_feature_importance_dummy(
                    processed_text, config.max_features
                )

            # åˆ›å»ºæ–‡æœ¬ç‰‡æ®µ
            text_snippet = original_text[:TEXT_SNIPPET_LENGTH]
            if len(original_text) > TEXT_SNIPPET_LENGTH:
                text_snippet += "..."

            # ä¼°ç®—tokenæ•°é‡ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
            token_count = len(processed_text.split()) if processed_text else 0

            result = ClassificationResult(
                index=i,
                text_snippet=text_snippet,
                prediction=prediction,
                probability=probability,
                top_predictions=top_predictions,
                feature_importance=feature_importance,
                text_length=len(original_text),
                token_count=token_count,
                warnings=text_warnings[i],
            )

            results.append(result)

        return results

    def _get_top_k_predictions(
        self, all_probs: dict[str, float], top_k: int
    ) -> list[ClassificationLabel]:
        """
        è·å–Top-Ké¢„æµ‹ç»“æœ.

        Args:
            all_probs: æ‰€æœ‰ç±»åˆ«æ¦‚ç‡å­—å…¸
            top_k: Top-Kæ•°é‡

        Returns:
            Top-Kåˆ†ç±»æ ‡ç­¾åˆ—è¡¨
        """
        # æŒ‰æ¦‚ç‡é™åºæ’åº
        sorted_probs = sorted(all_probs.items(), key=lambda x: x[1], reverse=True)

        top_k_results = []
        for rank, (label, prob) in enumerate(sorted_probs[:top_k], start=1):
            top_k_results.append(
                ClassificationLabel(
                    label=label,
                    probability=round(prob, 4),  # ä¿ç•™4ä½å°æ•°
                    rank=rank,
                )
            )

        return top_k_results

    def _get_feature_importance_dummy(
        self, text: str, max_features: int
    ) -> list[FeatureImportance]:
        """
        è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆDummyå®ç°ï¼Œè¿”å›æ–‡æœ¬ä¸­çš„è¯è¯­ï¼‰.

        æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»æ¨¡å‹ä¸­æå–çœŸå®çš„ç‰¹å¾é‡è¦æ€§ã€‚
        åœ¨çœŸå®MLflowæ¨¡å‹ä¸­ï¼Œéœ€è¦è®¿é—®æ¨¡å‹å†…éƒ¨çš„é¢„å¤„ç†å™¨å’Œç‰¹å¾å·¥ç¨‹å™¨ã€‚

        Args:
            text: æ–‡æœ¬å†…å®¹
            max_features: æœ€å¤šè¿”å›Nä¸ªç‰¹å¾

        Returns:
            ç‰¹å¾é‡è¦æ€§åˆ—è¡¨
        """
        # ç®€å•åˆ†è¯ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨è®­ç»ƒæ—¶çš„é¢„å¤„ç†å™¨ï¼‰
        words = text.split()[:max_features]

        # ç”Ÿæˆæ¨¡æ‹Ÿçš„é‡è¦æ€§å¾—åˆ†
        features = []
        for i, word in enumerate(words):
            importance = 1.0 / (i + 1)  # ç®€å•é€’å‡
            features.append(
                FeatureImportance(
                    feature=word, importance=importance, contribution="positive"
                )
            )

        return features

    def _compute_summary(
        self, results: list[ClassificationResult], processing_time_ms: float
    ) -> PredictionSummary:
        """
        è®¡ç®—æ‰¹é‡é¢„æµ‹æ±‡æ€»ç»Ÿè®¡.

        Args:
            results: åˆ†ç±»ç»“æœåˆ—è¡¨
            processing_time_ms: å¤„ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            é¢„æµ‹æ±‡æ€»ç»Ÿè®¡
        """
        total_samples = len(results)

        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        probabilities = []
        warnings_count = 0

        for result in results:
            # ç»Ÿè®¡é¢„æµ‹ç±»åˆ«
            pred = result.prediction
            class_distribution[pred] = class_distribution.get(pred, 0) + 1

            # æ”¶é›†æ¦‚ç‡
            probabilities.append(result.probability)

            # ç»Ÿè®¡è­¦å‘Šæ•°
            warnings_count += len(result.warnings)

        # è®¡ç®—å¹³å‡æ¦‚ç‡
        avg_probability = (
            sum(probabilities) / len(probabilities) if probabilities else 0.0
        )

        return PredictionSummary(
            total_samples=total_samples,
            class_distribution=class_distribution,
            avg_probability=avg_probability,
            processing_time_ms=processing_time_ms,
            warnings_count=warnings_count,
        )

    def _build_metadata(
        self, config: PredictionConfig, request_time: str, execution_time_ms: float
    ) -> ResponseMetadata:
        """
        æ„é€ å“åº”å…ƒæ•°æ®.

        Args:
            config: é¢„æµ‹é…ç½®
            request_time: è¯·æ±‚æ—¶é—´
            execution_time_ms: æ‰§è¡Œè€—æ—¶

        Returns:
            å“åº”å…ƒæ•°æ®
        """
        model_uri = None
        if self.config.source == "mlflow":
            model_uri = self.config.mlflow_model_uri
        elif self.config.source == "local":
            model_uri = self.config.model_path

        return ResponseMetadata(
            model_uri=model_uri,
            model_source=self.config.source,
            config_used=config.model_dump(),
            request_time=request_time,
            execution_time_ms=execution_time_ms,
        )

    def _create_error_response(
        self,
        code: str,
        message: str,
        details: dict,
        request_time: str,
        execution_time_ms: float,
    ) -> PredictResponse:
        """
        åˆ›å»ºé”™è¯¯å“åº”.

        Args:
            code: é”™è¯¯ä»£ç 
            message: é”™è¯¯æ¶ˆæ¯
            details: é”™è¯¯è¯¦æƒ…
            request_time: è¯·æ±‚æ—¶é—´
            execution_time_ms: æ‰§è¡Œè€—æ—¶

        Returns:
            é”™è¯¯å“åº”
        """
        return PredictResponse(
            success=False,
            results=None,
            summary=None,
            metadata=ResponseMetadata(
                model_uri=None,
                model_source=self.config.source,
                config_used={},
                request_time=request_time,
                execution_time_ms=execution_time_ms,
            ),
            error=ErrorDetail(code=code, message=message, details=details),
        )

    @bentoml.api
    async def health(self) -> dict:
        """å¥åº·æ£€æŸ¥æ¥å£."""
        health_check_counter.inc()
        return {
            "status": "healthy",
            "model_source": self.config.source,
            "model_version": getattr(self.model, "version", "unknown"),
        }
