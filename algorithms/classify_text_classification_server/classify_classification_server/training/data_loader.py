"""æ•°æ®åŠ è½½æ¨¡å—"""
from pathlib import Path
from typing import Tuple, Optional
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from loguru import logger


def load_csv_dataset(file_path: str) -> pd.DataFrame:
    """åŠ è½½ CSV æ•°æ®é›†
    
    Args:
        file_path: CSV æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å« text å’Œ label åˆ—çš„ DataFrame
        
    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: ç¼ºå°‘å¿…éœ€çš„åˆ—
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    logger.info(f"ğŸ“ åŠ è½½æ•°æ®é›†: {file_path}")
    df = pd.read_csv(file_path)
    
    # æ£€æŸ¥å¿…éœ€çš„åˆ—
    required_columns = ["text", "label"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        raise ValueError(f"æ•°æ®é›†ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}. "
                        f"å½“å‰åˆ—: {df.columns.tolist()}")
    
    logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {len(df)} æ¡æ ·æœ¬, "
               f"åˆ—: {df.columns.tolist()}")
    
    # ç§»é™¤ç¼ºå¤±å€¼
    original_len = len(df)
    df = df.dropna(subset=["text", "label"])
    if len(df) < original_len:
        logger.warning(f"âš  ç§»é™¤äº† {original_len - len(df)} æ¡åŒ…å«ç¼ºå¤±å€¼çš„æ ·æœ¬")
    
    return df


def split_dataset(
    df: pd.DataFrame,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    
    Args:
        df: åŸå§‹æ•°æ®é›†
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
        
    Returns:
        (train_df, val_df, test_df) å…ƒç»„
        
    Raises:
        ValueError: æ¯”ä¾‹å’Œä¸ä¸º1
    """
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
        raise ValueError(f"æ•°æ®é›†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1ï¼Œå½“å‰: "
                        f"{train_ratio} + {val_ratio} + {test_ratio} = "
                        f"{train_ratio + val_ratio + test_ratio}")
    
    logger.info(f"ğŸ“Š åˆ†å‰²æ•°æ®é›†: train={train_ratio}, val={val_ratio}, test={test_ratio}")
    
    # ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼šåˆ†ç¦»å‡ºè®­ç»ƒé›†
    train_df, temp_df = train_test_split(
        df,
        train_size=train_ratio,
        random_state=random_state,
        stratify=df["label"]  # ä¿æŒç±»åˆ«åˆ†å¸ƒ
    )
    
    # ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šä»å‰©ä½™æ•°æ®ä¸­åˆ†ç¦»éªŒè¯é›†å’Œæµ‹è¯•é›†
    val_size_adjusted = val_ratio / (val_ratio + test_ratio)
    val_df, test_df = train_test_split(
        temp_df,
        train_size=val_size_adjusted,
        random_state=random_state,
        stratify=temp_df["label"]
    )
    
    logger.info(f"æ•°æ®é›†åˆ†å‰²å®Œæˆ: "
               f"train={len(train_df)}, val={len(val_df)}, test={len(test_df)}")
    
    # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
    logger.info(f"è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\n{train_df['label'].value_counts()}")
    logger.info(f"éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\n{val_df['label'].value_counts()}")
    logger.info(f"æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:\n{test_df['label'].value_counts()}")
    
    return train_df, val_df, test_df


def load_and_split(
    dataset_path: str,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """åŠ è½½å¹¶åˆ†å‰²æ•°æ®é›†
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. å•æ–‡ä»¶æ¨¡å¼ï¼šä¼ å…¥ CSV æ–‡ä»¶è·¯å¾„ï¼Œè‡ªåŠ¨åˆ†å‰²
    2. ç›®å½•æ¨¡å¼ï¼šä¼ å…¥ç›®å½•è·¯å¾„ï¼ŒåŠ è½½ train.csv, val.csv, test.csv
    
    Args:
        dataset_path: æ•°æ®é›†æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆä»…å•æ–‡ä»¶æ¨¡å¼ï¼‰
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»…å•æ–‡ä»¶æ¨¡å¼ï¼‰
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆä»…å•æ–‡ä»¶æ¨¡å¼ï¼‰
        random_state: éšæœºç§å­
        
    Returns:
        (train_df, val_df, test_df) å…ƒç»„
    """
    dataset_path = Path(dataset_path)
    
    # ç›®å½•æ¨¡å¼
    if dataset_path.is_dir():
        logger.info(f"ğŸ“ æ£€æµ‹åˆ°ç›®å½•æ¨¡å¼: {dataset_path}")
        
        # æ”¯æŒå¤šç§æ–‡ä»¶å‘½åæ ¼å¼
        train_file = dataset_path / "train_data.csv" if (dataset_path / "train_data.csv").exists() else dataset_path / "train.csv"
        val_file = dataset_path / "val_data.csv" if (dataset_path / "val_data.csv").exists() else dataset_path / "val.csv"
        test_file = dataset_path / "test_data.csv" if (dataset_path / "test_data.csv").exists() else dataset_path / "test.csv"
        
        if not train_file.exists():
            raise FileNotFoundError(f"è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨: {train_file}. "
                                   f"è¯·ç¡®ä¿ç›®å½•ä¸‹å­˜åœ¨ train.csv æˆ– train_data.csv")
        
        train_df = load_csv_dataset(str(train_file))
        
        # éªŒè¯é›†å’Œæµ‹è¯•é›†å¯é€‰
        val_df = load_csv_dataset(str(val_file)) if val_file.exists() else None
        test_df = load_csv_dataset(str(test_file)) if test_file.exists() else None
        
        if val_df is None:
            logger.warning("âš  æœªæ‰¾åˆ° val_data.csvï¼Œå°†ä»è®­ç»ƒé›†è‡ªåŠ¨åˆ’åˆ†")
            train_df, val_df = train_test_split(
                train_df,
                train_size=0.85,
                random_state=random_state,
                stratify=train_df["label"]
            )
        
        if test_df is None:
            logger.warning("âš  æœªæ‰¾åˆ° test_data.csvï¼Œå°†ä»è®­ç»ƒé›†è‡ªåŠ¨åˆ’åˆ†")
            test_df = val_df
        
        return train_df, val_df, test_df
    
    # å•æ–‡ä»¶æ¨¡å¼
    elif dataset_path.is_file():
        logger.info(f"ğŸ“„ æ£€æµ‹åˆ°æ–‡ä»¶æ¨¡å¼: {dataset_path}")
        logger.info("â„¹ å°†æŒ‰å›ºå®šæ¯”ä¾‹è‡ªåŠ¨åˆ’åˆ†ï¼ˆtrain=0.7, val=0.15, test=0.15ï¼‰")
        df = load_csv_dataset(str(dataset_path))
        return split_dataset(df, train_ratio, val_ratio, test_ratio, random_state)
    
    else:
        raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")


def encode_labels(
    train_labels: pd.Series,
    val_labels: Optional[pd.Series] = None,
    test_labels: Optional[pd.Series] = None
) -> Tuple[pd.Series, Optional[pd.Series], Optional[pd.Series], LabelEncoder]:
    """ç¼–ç æ ‡ç­¾ä¸ºæ•´æ•°
    
    Args:
        train_labels: è®­ç»ƒé›†æ ‡ç­¾
        val_labels: éªŒè¯é›†æ ‡ç­¾
        test_labels: æµ‹è¯•é›†æ ‡ç­¾
        
    Returns:
        (train_encoded, val_encoded, test_encoded, label_encoder) å…ƒç»„
    """
    logger.info(f"ç¼–ç æ ‡ç­¾ï¼Œç±»åˆ«æ•°: {train_labels.nunique()}")
    
    label_encoder = LabelEncoder()
    train_encoded = pd.Series(label_encoder.fit_transform(train_labels))
    
    val_encoded = None
    if val_labels is not None:
        val_encoded = pd.Series(label_encoder.transform(val_labels))
    
    test_encoded = None
    if test_labels is not None:
        test_encoded = pd.Series(label_encoder.transform(test_labels))
    
    # æ˜¾ç¤ºæ ‡ç­¾æ˜ å°„
    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
    logger.info(f"æ ‡ç­¾æ˜ å°„: {label_mapping}")
    
    return train_encoded, val_encoded, test_encoded, label_encoder
