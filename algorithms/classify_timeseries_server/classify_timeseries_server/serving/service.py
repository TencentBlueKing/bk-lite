"""BentoML service definition."""

import bentoml
from loguru import logger
import mlflow
import os
import time

import mlflow.sklearn

from .config import get_model_config
from .exceptions import ModelInferenceError
from .metrics import (
    health_check_counter,
    model_load_counter,
    prediction_counter,
    prediction_duration,
)
from .models import load_model
from .schemas import PredictRequest, PredictResponse


@bentoml.service(
    name=f"{{project_name}}_service",
    traffic={"timeout": 30},
)
class MLService:
    """æœºå™¨å­¦ä¹ æ¨¡å‹æœåŠ¡."""

    @bentoml.on_deployment
    def setup() -> None:
        """
        éƒ¨ç½²æ—¶æ‰§è¡Œä¸€æ¬¡çš„å…¨å±€åˆå§‹åŒ–.

        ç”¨äºé¢„çƒ­ç¼“å­˜ã€ä¸‹è½½èµ„æºç­‰å…¨å±€æ“ä½œ.
        ä¸æ¥æ”¶ self å‚æ•°,ç±»ä¼¼é™æ€æ–¹æ³•.
        """
        logger.info("=== Deployment setup started ===")
        # å¯ä»¥åœ¨è¿™é‡Œåšå…¨å±€åˆå§‹åŒ–,ä¾‹å¦‚:
        # - é¢„çƒ­æ¨¡å‹ç¼“å­˜
        # - ä¸‹è½½å…±äº«èµ„æº
        # - åˆå§‹åŒ–å…¨å±€è¿æ¥æ± 
        logger.info("=== Deployment setup completed ===")

    def __init__(self) -> None:
        """åˆå§‹åŒ–æœåŠ¡,åŠ è½½é…ç½®å’Œæ¨¡å‹."""
        logger.info("Service instance initializing...")
        self.config = get_model_config()
        logger.info(f"Config loaded: {self.config}")

        # å¯åŠ¨æ—¶éªŒè¯é…ç½®ï¼ˆå¿«é€Ÿå¤±è´¥ï¼‰
        self._validate_config()

        # å°è¯•åŠ è½½æ¨¡å‹
        try:
            load_start = time.time()
            self.model = load_model(self.config)
            load_time = time.time() - load_start
            
            model_load_counter.labels(
                source=self.config.source, status="success").inc()
            logger.info(f"â±ï¸  Model loaded successfully in {load_time:.3f}s: {self.config.mlflow_model_uri or 'local/dummy'}")
            
        except Exception as e:
            model_load_counter.labels(
                source=self.config.source, status="failure").inc()
            logger.error(f"âŒ Failed to load model: {e}", exc_info=True)
            
            # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦å…è®¸é™çº§åˆ° DummyModel
            allow_fallback = os.getenv("ALLOW_DUMMY_FALLBACK", "false").lower() == "true"
            
            if allow_fallback:
                from .models.dummy_model import DummyModel
                logger.warning("âš ï¸  ALLOW_DUMMY_FALLBACK=true, using DummyModel as fallback")
                self.model = DummyModel()
                model_load_counter.labels(
                    source="dummy_fallback", status="success").inc()
            else:
                logger.error(
                    "Model loading failed and fallback is disabled. "
                    "Set ALLOW_DUMMY_FALLBACK=true to enable DummyModel fallback."
                )
                raise RuntimeError(
                    f"Failed to load model from source '{self.config.source}'. "
                    "Service cannot start without a valid model. "
                    "Enable fallback with ALLOW_DUMMY_FALLBACK=true for development/testing."
                ) from e
    
    def _validate_config(self) -> None:
        """éªŒè¯æ¨¡å‹é…ç½®ï¼ˆå¯åŠ¨æ—¶å¿«é€Ÿæ£€æŸ¥ï¼‰."""
        from pathlib import Path
        
        logger.info("Validating model configuration...")
        
        if self.config.source == "local":
            # æœ¬åœ°æ¨¡å¼ï¼šæ£€æŸ¥è·¯å¾„å’Œå…³é”®æ–‡ä»¶
            if not self.config.model_path:
                raise ValueError(
                    "MODEL_SOURCE is 'local' but MODEL_PATH is not set. "
                    "Please set MODEL_PATH environment variable to a valid MLflow model directory."
                )
            
            model_path = Path(self.config.model_path)
            
            if not model_path.exists():
                raise ValueError(
                    f"MODEL_PATH does not exist: {model_path}. "
                    "Ensure the path is correct and accessible."
                )
            
            if not model_path.is_dir():
                raise ValueError(
                    f"MODEL_PATH must be a directory (MLflow model format), got: {model_path}. "
                    "Example: /path/to/mlruns/1/<run_id>/artifacts/model/"
                )
            
            if not (model_path / "MLmodel").exists():
                raise ValueError(
                    f"Invalid MLflow model at {model_path}: MLmodel file not found. "
                    "Ensure the path points to a valid MLflow model directory containing MLmodel file."
                )
            
            logger.info(f"âœ… Local model config validated: {model_path}")
        
        elif self.config.source == "mlflow":
            # MLflow Registry æ¨¡å¼ï¼šæ£€æŸ¥ URI
            if not self.config.mlflow_model_uri:
                raise ValueError(
                    "MODEL_SOURCE is 'mlflow' but MLFLOW_MODEL_URI is not set. "
                    "Example: models:/model_name/version or models:/model_name/Production"
                )
            
            logger.info(f"âœ… MLflow model config validated: {self.config.mlflow_model_uri}")
        
        elif self.config.source == "dummy":
            logger.info("âœ… Using dummy model (no validation needed)")
        
        else:
            logger.warning(f"âš ï¸  Unknown model source: {self.config.source}, will attempt to load")

    @bentoml.on_shutdown
    def cleanup(self) -> None:
        """
        æœåŠ¡å…³é—­æ—¶çš„æ¸…ç†æ“ä½œ.

        ç”¨äºé‡Šæ”¾èµ„æºã€å…³é—­è¿æ¥ç­‰.
        """
        logger.info("=== Service shutdown: cleaning up resources ===")
        # æ¸…ç†é€»è¾‘,ä¾‹å¦‚:
        # - å…³é—­æ•°æ®åº“è¿æ¥
        # - ä¿å­˜ç¼“å­˜çŠ¶æ€
        # - é‡Šæ”¾ GPU æ˜¾å­˜
        logger.info("=== Cleanup completed ===")

    @bentoml.api
    async def predict(
        self,
        data: list,
        config: dict
    ) -> PredictResponse:
        """
        é¢„æµ‹æ¥å£.

        Args:
            data: å†å²æ—¶é—´åºåˆ—æ•°æ®ç‚¹åˆ—è¡¨
            config: é¢„æµ‹é…ç½®ï¼ˆåŒ…å« stepsï¼‰

        Returns:
            é¢„æµ‹å“åº”
        """
        import time
        import pandas as pd
        
        request_start = time.time()
        
        # æ„é€  PredictRequest å¯¹è±¡è¿›è¡ŒéªŒè¯
        from .schemas import TimeSeriesPoint, PredictionConfig, ResponseMetadata, ErrorDetail
        try:
            data_points = [TimeSeriesPoint(**point) for point in data]
            pred_config = PredictionConfig(**config)
            request = PredictRequest(data=data_points, config=pred_config)
        except Exception as e:
            logger.error(f"Request validation failed: {e}")
            # è¿”å›éªŒè¯å¤±è´¥å“åº”
            return PredictResponse(
                success=False,
                history=None,
                prediction=None,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri,
                    prediction_steps=0,
                    input_data_points=len(data) if data else 0,
                    input_frequency=None,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=ErrorDetail(
                    code="E1000",
                    message=f"è¯·æ±‚æ ¼å¼éªŒè¯å¤±è´¥: {str(e)}",
                    details={"error_type": type(e).__name__}
                )
            )
        
        logger.info(f"Received prediction request: steps={request.config.steps}, data_points={len(request.data)}")

        try:
            # è½¬æ¢å†å²æ•°æ®
            history = request.to_series()
            steps = request.config.steps
            
            # æ¨æ–­é¢‘ç‡ï¼ˆä¸¥æ ¼éªŒè¯ï¼‰
            inferred_freq = pd.infer_freq(history.index)
            if inferred_freq is None:
                raise ValueError("æ— æ³•æ¨æ–­è¾“å…¥æ•°æ®çš„æ—¶é—´é¢‘ç‡ï¼Œè¯·æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦è§„åˆ™")
            
            logger.info(f"Detected frequency: {inferred_freq}")
            
            # æ‰§è¡Œé¢„æµ‹ï¼ˆæ·»åŠ æ¨¡å‹æ¥æºä¿¡æ¯ï¼‰
            model_info = f"source={self.config.source}, type={type(self.model).__name__}"
            if self.config.source == "local":
                model_info += f", path={self.config.model_path}"
            elif self.config.source == "mlflow":
                model_info += f", uri={self.config.mlflow_model_uri}"
            
            logger.info(f"ğŸ”® Executing prediction with model [{model_info}]")
            
            # æ ¹æ®é˜ˆå€¼å†³å®šé¢„æµ‹æ¨¡å¼
            threshold = request.config.threshold
            if threshold and steps > threshold:
                logger.info(f"é¢„æµ‹æ­¥æ•°({steps})è¶…è¿‡é˜ˆå€¼({threshold}), ä½¿ç”¨æ»šåŠ¨é¢„æµ‹æ¨¡å¼")
                predict_mode = 'rolling'
            else:
                predict_mode = 'recursive'
                if threshold and steps > threshold * 0.8:
                    logger.warning(f"é¢„æµ‹æ­¥æ•°({steps})æ¥è¿‘é˜ˆå€¼({threshold}), å»ºè®®ä½¿ç”¨æ»šåŠ¨é¢„æµ‹")
            
            predict_start = time.time()
            prediction_values = self.model.predict({
                'history': history,
                'steps': steps,
                'mode': predict_mode,
                'threshold': threshold
            })
            predict_time = time.time() - predict_start
            logger.info(f"â±ï¸  Prediction executed in {predict_time:.3f}s (mode={predict_mode})")
            
            # ç”Ÿæˆé¢„æµ‹æ—¶é—´æˆ³
            last_timestamp = history.index[-1]
            predicted_points = []
            for i in range(1, steps + 1):
                next_ts = last_timestamp + i * pd.tseries.frequencies.to_offset(inferred_freq)
                predicted_points.append(TimeSeriesPoint(
                    timestamp=next_ts.isoformat(),
                    value=float(prediction_values[i-1])
                ))
            
            # æ„é€ æˆåŠŸå“åº”
            response = PredictResponse(
                success=True,
                history=request.data,
                prediction=predicted_points,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri,
                    prediction_steps=steps,
                    input_data_points=len(request.data),
                    input_frequency=inferred_freq,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=None
            )
            
            total_time = time.time() - request_start
            logger.info(f"â±ï¸  Total request time: {total_time:.3f}s")
            
            return response
            
        except ValueError as e:
            # éªŒè¯é”™è¯¯ï¼ˆé¢‘ç‡æ¨æ–­å¤±è´¥ç­‰ï¼‰
            logger.error(f"Validation error: {e}")
            return PredictResponse(
                success=False,
                history=None,
                prediction=None,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri,
                    prediction_steps=0,
                    input_data_points=len(data) if data else 0,
                    input_frequency=None,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=ErrorDetail(
                    code="E1001",
                    message=str(e),
                    details={"error_type": "ValidationError"}
                )
            )
            
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼ˆæ¨¡å‹é¢„æµ‹å¤±è´¥ç­‰ï¼‰
            logger.error(f"Prediction failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return PredictResponse(
                success=False,
                history=None,
                prediction=None,
                metadata=ResponseMetadata(
                    model_uri=self.config.mlflow_model_uri,
                    prediction_steps=0,
                    input_data_points=len(data) if data else 0,
                    input_frequency=None,
                    execution_time_ms=(time.time() - request_start) * 1000
                ),
                error=ErrorDetail(
                    code="E2002",
                    message=f"æ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}",
                    details={"error_type": type(e).__name__}
                )
            )

    @bentoml.api
    async def health(self) -> dict:
        """å¥åº·æ£€æŸ¥æ¥å£."""
        health_check_counter.inc()
        return {
            "status": "healthy",
            "model_source": self.config.source,
            "model_version": getattr(self.model, "version", "unknown"),
        }
