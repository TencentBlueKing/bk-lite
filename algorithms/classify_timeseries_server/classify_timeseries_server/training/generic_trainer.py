"""é€šç”¨æ—¶é—´åºåˆ—æ¨¡å‹è®­ç»ƒå™¨."""

from pathlib import Path
from typing import Dict, Any, Optional, Callable
import pandas as pd
import numpy as np
import mlflow
from loguru import logger
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib
from hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK

from .base_model import BaseTimeSeriesModel
from .mlflow_utils import MLFlowUtils
from .algorithms.base_algorithm import BaseTimeSeriesAlgorithm


class TimeSeriesTrainer(BaseTimeSeriesModel):
    """
    é€šç”¨æ—¶é—´åºåˆ—è®­ç»ƒå™¨.
    
    é€šè¿‡ç»„åˆæ¨¡å¼æ³¨å…¥å…·ä½“ç®—æ³•å®ç°ï¼Œå®ç°ç®—æ³•å¯æ’æ‹”.
    """
    
    def __init__(self, algorithm: BaseTimeSeriesAlgorithm):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨.
        
        Args:
            algorithm: å…·ä½“ç®—æ³•å®ç°ï¼ˆSARIMAã€Prophetã€LSTM ç­‰ï¼‰
        """
        super().__init__()
        self.algorithm = algorithm
    
    def build_model(self, train_params: dict):
        """æ„å»ºæ¨¡å‹ï¼ˆå§”æ‰˜ç»™ç®—æ³•å®ç°ï¼‰."""
        return train_params
    
    def train(
        self,
        model_name: str,
        train_dataframe: pd.DataFrame,
        val_dataframe: Optional[pd.DataFrame] = None,
        test_dataframe: Optional[pd.DataFrame] = None,
        train_config: dict = {},
        mlflow_tracking_uri: Optional[str] = None,
        experiment_name: str = "timeseries_training",
        test_size: float = 0.2,
        max_evals: int = 0,
        optimization_metric: str = "rmse",
        **kwargs
    ) -> Dict[str, Any]:
        """
        é€šç”¨è®­ç»ƒæµç¨‹.
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æ•°æ®é¢„å¤„ç†ï¼ˆé€šç”¨ï¼‰
        2. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼Œæ ¹æ® max_evals å’Œ train_configï¼‰
        3. æ¨¡å‹è®­ç»ƒï¼ˆå§”æ‰˜ç»™ç®—æ³•ï¼‰
        4. è¯„ä¼°é¢„æµ‹ï¼ˆé€šç”¨ï¼‰
        5. MLflow è®°å½•ï¼ˆé€šç”¨ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            train_dataframe: è®­ç»ƒæ•°æ®,åŒ…å« 'date' å’Œ 'value' åˆ—
            val_dataframe: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºè¶…å‚æ•°ä¼˜åŒ–å’ŒéªŒè¯é›†è¯„ä¼°ï¼‰
            test_dataframe: æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»è®­ç»ƒæ•°æ®åˆ†å‰²ï¼‰
            train_config: è®­ç»ƒé…ç½®ï¼Œæ”¯æŒå›ºå®šå€¼æˆ–æœç´¢ç©ºé—´å®šä¹‰
                - å›ºå®šå€¼æ¨¡å¼: {"order": [1,1,1], "seasonal_order": [1,1,1,12]}
                - æœç´¢ç©ºé—´æ¨¡å¼: {"order_p": {"type": "randint", "min": 0, "max": 2}, ...}
            mlflow_tracking_uri: MLflow tracking åœ°å€
            experiment_name: å®éªŒåç§°
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            max_evals: è¶…å‚æ•°ä¼˜åŒ–è½®æ¬¡ (0=ä¸ä¼˜åŒ–ï¼Œä½¿ç”¨å›ºå®šå€¼; >0=ä»train_configæ„å»ºæœç´¢ç©ºé—´)
            optimization_metric: ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡ (rmse/mae/mape)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸ï¼ŒåŒ…å« model, test_metrics, run_id, frequency, best_params
        """
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.algorithm.algorithm_name} æ¨¡å‹")
        
        # è®¾ç½® MLflow
        MLFlowUtils.setup_experiment(mlflow_tracking_uri, experiment_name)
        
        # æ•°æ®é¢„å¤„ç†(é€šç”¨é€»è¾‘)
        logger.info("ğŸ“Š æ•°æ®é¢„å¤„ç†ä¸­...")
        train_df_prep, frequency = self.preprocess(train_dataframe, None)
        self.frequency = frequency
        
        ts = train_df_prep.set_index('date')['value']
        
        # å¤„ç†éªŒè¯é›†
        val_data = None
        if val_dataframe is not None and not val_dataframe.empty:
            val_df_prep, _ = self.preprocess(val_dataframe, frequency)
            val_data = val_df_prep.set_index('date')['value']
            logger.info(f"éªŒè¯é›†å¤§å°: {len(val_data)}")
        
        # åˆ†å‰²æ•°æ®ï¼ˆé€šç”¨é€»è¾‘ï¼‰
        if test_dataframe is not None and not test_dataframe.empty:
            test_df_prep, _ = self.preprocess(test_dataframe, frequency)
            test_data = test_df_prep.set_index('date')['value']
            train_data = ts
        else:
            split_point = int(len(ts) * (1 - test_size))
            train_data = ts[:split_point]
            test_data = ts[split_point:]
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_data)}, æµ‹è¯•é›†å¤§å°: {len(test_data)}")
        
        # è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ ¹æ® max_evals å’Œ train_config åˆ¤æ–­ï¼‰
        best_config = train_config
        optimization_history = []
        enable_hyperparam_tuning = max_evals > 0
        
        if enable_hyperparam_tuning:
            # ä» train_config æ„å»ºæœç´¢ç©ºé—´
            hyperparam_space = self._build_search_space_from_config(train_config)
            
            if not hyperparam_space:
                logger.warning(f"max_evals={max_evals} ä½† train_config ä¸åŒ…å«æœç´¢èŒƒå›´å®šä¹‰ï¼Œå°†ä½¿ç”¨å›ºå®šå€¼")
                enable_hyperparam_tuning = False
            else:
                logger.info(f"ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œæœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evals}, ä¼˜åŒ–æŒ‡æ ‡: {optimization_metric}")
                logger.info(f"æœç´¢ç©ºé—´å‚æ•°: {list(hyperparam_space.keys())}")
                
                # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†åˆ†å‰²
                opt_val_data = val_data
                if opt_val_data is None:
                    val_split = int(len(train_data) * 0.8)
                    opt_train_data = train_data[:val_split]
                    opt_val_data = train_data[val_split:]
                    logger.info(f"ä»è®­ç»ƒé›†åˆ†å‰²éªŒè¯é›†: è®­ç»ƒ {len(opt_train_data)}, éªŒè¯ {len(opt_val_data)}")
                else:
                    opt_train_data = train_data
                
                best_config, optimization_history = self._tune_hyperparams(
                    opt_train_data,
                    opt_val_data,
                    train_config,
                    hyperparam_space,
                    max_evals,
                    optimization_metric
                )
                
                logger.info(f"âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œæœ€ä¼˜é…ç½®: {best_config}")
        else:
            logger.info(f"è·³è¿‡è¶…å‚æ•°ä¼˜åŒ– (max_evals={max_evals})ï¼Œä½¿ç”¨æä¾›çš„ train_config")
        
        # å¼€å§‹ MLflow run
        with mlflow.start_run() as run:
            # è®°å½•é€šç”¨å‚æ•°
            params = {
                'algorithm': self.algorithm.algorithm_name,
                'train_size': len(train_data),
                'test_size': len(test_data),
                'frequency': frequency or 'unknown',
            }
            
            # æ·»åŠ ç®—æ³•ç‰¹å®šçš„è¶…å‚æ•°ï¼ˆå±•å¹³å¤„ç†ï¼‰
            flattened_params = self.algorithm.flatten_hyperparams(best_config)
            params.update(flattened_params)
            
            # æ·»åŠ ä¼˜åŒ–ç›¸å…³å‚æ•°
            params.update({
                'max_evals': max_evals,
                'optimization_enabled': enable_hyperparam_tuning,
            })
            if enable_hyperparam_tuning:
                params['optimization_metric'] = optimization_metric
            
            logger.info(f"{self.algorithm.algorithm_name} å‚æ•°: {flattened_params}")
            MLFlowUtils.log_params_batch(params)
            
            # è®­ç»ƒæ¨¡å‹(å§”æ‰˜ç»™ç®—æ³•å®ç°ï¼Œä½¿ç”¨æœ€ä¼˜é…ç½®)
            logger.info(f"ğŸ”§ æ‹Ÿåˆ {self.algorithm.algorithm_name} æ¨¡å‹ä¸­...")
            try:
                fitted_model = self.algorithm.fit(train_data, best_config)
                logger.info("âœ… æ¨¡å‹æ‹ŸåˆæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
                raise
            
            # é¢„æµ‹(å§”æ‰˜ç»™ç®—æ³•å®ç°)
            logger.info("ğŸ“ˆ ç”Ÿæˆé¢„æµ‹ç»“æœä¸­...")
            predictions = self.algorithm.predict(fitted_model, len(test_data))
            
            # è¯„ä¼°éªŒè¯é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            val_metrics = {}
            if val_data is not None:
                logger.info("ğŸ“Š éªŒè¯é›†è¯„ä¼°ä¸­...")
                val_predictions = self.algorithm.predict(fitted_model, len(val_data))
                val_values = val_data.values if isinstance(val_data, pd.Series) else val_data
                val_metrics = self._calculate_metrics(val_values, val_predictions)
                logger.info(f"ğŸ“Š éªŒè¯é›†æŒ‡æ ‡: {val_metrics}")
                MLFlowUtils.log_metrics_batch(val_metrics, prefix="val_")
            
            # è¯„ä¼°ï¼ˆé€šç”¨é€»è¾‘ï¼‰
            test_values = test_data.values if isinstance(test_data, pd.Series) else test_data
            test_metrics = self._calculate_metrics(test_values, predictions)
            
            # ç®—æ³•ç‰¹å®šæŒ‡æ ‡
            additional_metrics = self.algorithm.get_additional_metrics(fitted_model)
            test_metrics.update(additional_metrics)
            
            logger.info(f"ğŸ“Š æµ‹è¯•æŒ‡æ ‡: {test_metrics}")
            MLFlowUtils.log_metrics_batch(test_metrics, prefix="test_")
            
            # ä¿å­˜å›¾è¡¨(é€šç”¨é€»è¾‘)
            self._save_prediction_plot(test_values, predictions, test_metrics['rmse'])
            
            # ä¿å­˜æ¨¡å‹(ç®—æ³•æä¾›åŒ…è£…å™¨)
            logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° MLflow...")
            wrapped_model = self.algorithm.get_model_wrapper(fitted_model, frequency or 'D')
            
            MLFlowUtils.log_model(
                model=wrapped_model,
                artifact_path="model",
                registered_model_name=model_name,
                pip_requirements=self.algorithm.get_pip_requirements(),
            )
            
            # é¢å¤–ä¿å­˜åŸå§‹æ¨¡å‹
            model_path = Path('model.pkl')
            try:
                joblib.dump(fitted_model, model_path)
                MLFlowUtils.log_artifact(str(model_path))
            finally:
                # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if model_path.exists():
                    model_path.unlink()
            
            run_id = run.info.run_id
            logger.info(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ. Run ID: {run_id}")
            
            if model_name:
                logger.info(f"âœ… æ¨¡å‹å·²æ³¨å†Œä¸º: {model_name}")
        
        return {
            "model": fitted_model,
            "test_metrics": test_metrics,
            "val_metrics": val_metrics,
            "run_id": run_id,
            "frequency": frequency,
            "best_params": best_config,
            "optimization_history": optimization_history,
        }
    
    def _tune_hyperparams(
        self,
        train_data: pd.Series,
        val_data: pd.Series,
        base_config: Dict[str, Any],
        hyperparam_space: Dict,
        max_evals: int = 50,
        optimization_metric: str = "rmse"
    ) -> tuple[Dict[str, Any], list]:
        """
        ä½¿ç”¨ Hyperopt è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–.
        
        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            base_config: åŸºç¡€é…ç½®ï¼ˆå›ºå®šå€¼å‚æ•°ï¼‰
            hyperparam_space: è¶…å‚æ•°æœç´¢ç©ºé—´
            max_evals: æœ€å¤§è¯„ä¼°æ¬¡æ•°
            optimization_metric: ä¼˜åŒ–æŒ‡æ ‡
            
        Returns:
            (æœ€ä¼˜é…ç½®, ä¼˜åŒ–å†å²)
        """
        trials = Trials()
        history = []
        
        def objective(params_raw):
            """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
            try:
                # è¯„ä¼°è¶…å‚æ•°
                params = space_eval(hyperparam_space, params_raw)
                
                # åˆå¹¶æœç´¢å‚æ•°å’Œå›ºå®šå‚æ•°
                full_params = {}
                for k, v in base_config.items():
                    if not k.startswith('_') and not isinstance(v, dict):
                        full_params[k] = v
                full_params.update(params)
                
                # è®­ç»ƒæ¨¡å‹
                model = self.algorithm.fit(train_data, full_params)
                
                # éªŒè¯é›†é¢„æµ‹
                val_predictions = self.algorithm.predict(model, len(val_data))
                val_values = val_data.values if isinstance(val_data, pd.Series) else val_data
                
                # è®¡ç®—ç›®æ ‡æŒ‡æ ‡
                metrics = self._calculate_metrics(val_values, val_predictions)
                score = metrics.get(optimization_metric, metrics['rmse'])
                
                # è®°å½•å†å²
                history.append({
                    'params': full_params,
                    'score': score,
                    'metrics': metrics
                })
                
                # å®šæœŸæ—¥å¿—
                if len(history) % 10 == 0:
                    logger.info(f"ç¬¬ {len(history)} æ¬¡è¯„ä¼° - {optimization_metric}: {score:.4f}")
                
                # Hyperopt æœ€å°åŒ– loss
                return {
                    'loss': float(score),
                    'status': STATUS_OK,
                    'params': full_params
                }
                
            except Exception as e:
                logger.warning(f"è¶…å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
                return {
                    'loss': float('inf'),
                    'status': STATUS_OK
                }
        
        # è¿è¡Œä¼˜åŒ–
        best_params_raw = fmin(
            fn=objective,
            space=hyperparam_space,
            algo=tpe.suggest,
            max_evals=max_evals,
            trials=trials,
            rstate=np.random.default_rng(2025),
            verbose=False
        )
        
        best_params = space_eval(hyperparam_space, best_params_raw)
        
        # åˆå¹¶å›ºå®šå‚æ•°
        full_best_params = {}
        for k, v in base_config.items():
            if not k.startswith('_') and not isinstance(v, dict):
                full_best_params[k] = v
        full_best_params.update(best_params)
        
        # æ‰¾åˆ°æœ€ä¼˜ç»“æœ
        best_result = min(history, key=lambda x: x['score'])
        logger.info(f"æœ€ä¼˜ {optimization_metric}: {best_result['score']:.4f}")
        
        return full_best_params, history
    
    def _build_search_space_from_config(self, train_config: Dict[str, Any]) -> Optional[Dict]:
        """
        ä»è®­ç»ƒé…ç½®æ„å»º Hyperopt æœç´¢ç©ºé—´.
        
        æ”¯æŒçš„é…ç½®æ ¼å¼:
        1. å›ºå®šå€¼: {"order": [1, 1, 1], "trend": "c"}
        2. æœç´¢èŒƒå›´: {"order_p": {"type": "randint", "min": 0, "max": 2}}
        
        Args:
            train_config: è®­ç»ƒé…ç½®å­—å…¸
            
        Returns:
            Hyperopt æœç´¢ç©ºé—´å­—å…¸ï¼Œå¦‚æœä¸åŒ…å«æœç´¢èŒƒå›´åˆ™è¿”å› None
        """
        space = {}
        has_search_space = False
        
        for key, value in train_config.items():
            # è·³è¿‡æ³¨é‡Šå’Œæ¨¡å¼å­—æ®µ
            if key.startswith('_'):
                continue
            
            # å¦‚æœæ˜¯å­—å…¸ä¸”åŒ…å« type å­—æ®µï¼Œè¯´æ˜æ˜¯æœç´¢èŒƒå›´å®šä¹‰
            if isinstance(value, dict) and 'type' in value:
                has_search_space = True
                param_type = str(value.get('type', '')).lower()
                
                if param_type == 'randint':
                    vmin = int(value['min'])
                    vmax = int(value['max'])
                    space[key] = hp.randint(key, vmax - vmin + 1) + vmin
                    
                elif param_type == 'uniform':
                    vmin = float(value['min'])
                    vmax = float(value['max'])
                    space[key] = hp.uniform(key, vmin, vmax)
                    
                elif param_type == 'loguniform':
                    vmin = float(value['min'])
                    vmax = float(value['max'])
                    space[key] = hp.loguniform(key, np.log(vmin), np.log(vmax))
                    
                elif param_type == 'choice':
                    choices = value.get('choice', value.get('choices', []))
                    # å¤„ç†å­—ç¬¦ä¸²è½¬æ¢
                    processed_choices = []
                    for c in choices:
                        if isinstance(c, str):
                            lc = c.strip().lower()
                            if lc == 'none':
                                processed_choices.append(None)
                            elif lc == 'true':
                                processed_choices.append(True)
                            elif lc == 'false':
                                processed_choices.append(False)
                            else:
                                processed_choices.append(c)
                        else:
                            processed_choices.append(c)
                    space[key] = hp.choice(key, processed_choices)
                    
                elif param_type == 'choice_list':
                    # æ”¯æŒåˆ—è¡¨é€‰æ‹©ï¼Œå¦‚ [[0,1,0], [1,1,1], [2,1,2]]
                    choices = value.get('choices', [])
                    space[key] = hp.choice(key, [tuple(c) if isinstance(c, list) else c for c in choices])
                    
                else:
                    logger.warning(f"ä¸æ”¯æŒçš„å‚æ•°ç±»å‹: {param_type} for {key}")
            
            # å¦åˆ™æ˜¯å›ºå®šå€¼ï¼Œä¸æ·»åŠ åˆ°æœç´¢ç©ºé—´
        
        if not has_search_space:
            return None
        
        logger.info(f"ä»é…ç½®æ„å»ºæœç´¢ç©ºé—´: {list(space.keys())}")
        return space
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—é€šç”¨è¯„ä¼°æŒ‡æ ‡.
        
        Args:
            y_true: çœŸå®å€¼
            y_pred: é¢„æµ‹å€¼
            
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        
        # è®¡ç®— MAPEï¼Œé¿å…é™¤é›¶é”™è¯¯
        mape_values = np.abs((y_true - y_pred) / y_true)
        mape_values = mape_values[np.isfinite(mape_values)]
        mape = np.mean(mape_values) * 100 if len(mape_values) > 0 else 0.0
        
        return {
            'mse': float(mse),
            'rmse': float(rmse),
            'mae': float(mae),
            'mape': float(mape),
        }
    
    def _save_prediction_plot(self, y_true: np.ndarray, y_pred: np.ndarray, rmse: float):
        """
        ä¿å­˜é¢„æµ‹å›¾è¡¨.
        
        Args:
            y_true: çœŸå®å€¼
            y_pred: é¢„æµ‹å€¼
            rmse: RMSE æŒ‡æ ‡
        """
        try:
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            plot_index = range(len(y_true))
            
            plt.figure(figsize=(12, 6))
            plt.plot(plot_index, y_true, label='Actual', marker='o', markersize=3, alpha=0.7)
            plt.plot(plot_index, y_pred, label='Predicted', marker='x', markersize=3, alpha=0.7)
            plt.xlabel('Time Steps')
            plt.ylabel('Value')
            plt.title(f'{self.algorithm.algorithm_name} Predictions vs Actual (RMSE: {rmse:.2f})')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            plot_path = Path('predictions_plot.png')
            plt.savefig(plot_path, dpi=100)
            MLFlowUtils.log_artifact(str(plot_path))
            plot_path.unlink()
            plt.close()
            
            logger.info("ğŸ“Š é¢„æµ‹å›¾è¡¨å·²ä¿å­˜")
        except Exception as e:
            logger.warning(f"âš ï¸  åˆ›å»ºå›¾è¡¨å¤±è´¥: {e}")
