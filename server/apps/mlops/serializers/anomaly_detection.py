from apps.core.utils.serializers import AuthSerializer
from apps.mlops.models.anomaly_detection import *
from rest_framework import serializers


class AnomalyDetectionDatasetSerializer(AuthSerializer):
    """å¼‚å¸¸æ£€æµ‹æ•°æ®é›†åºåˆ—åŒ–å™¨"""
    permission_key = "dataset.anomaly_detection_dataset"

    class Meta:
        model = AnomalyDetectionDataset
        fields = "__all__"


class AnomalyDetectionTrainDataSerializer(AuthSerializer):

    permission_key = "dataset.anomaly_detection_train_data"

    class Meta:
        model = AnomalyDetectionTrainData
        fields = "__all__"  # å…è®¸æ–°å¢æ—¶åŒ…å«æ‰€æœ‰å­—æ®µ
        extra_kwargs = {
            'name': {'required': False},
            'train_data': {'required': False},
            'dataset': {'required': False},
        }

    def __init__(self, *args, **kwargs):
        """
        åˆå§‹åŒ–åºåˆ—åŒ–å™¨ï¼Œä»è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è·å– include_train_data å‚æ•°
        """
        super().__init__(*args, **kwargs)
        request = self.context.get('request')
        if request:
            self.include_train_data = request.query_params.get('include_train_data', 'false').lower() == 'true'
            self.include_metadata = request.query_params.get('include_metadata', 'false').lower() == 'true'
        else:
            self.include_train_data = False
            self.include_metadata = False

    def to_representation(self, instance):
        """
        è‡ªå®šä¹‰è¿”å›æ•°æ®ï¼Œæ ¹æ® include_train_data å‚æ•°åŠ¨æ€æ§åˆ¶ train_data å­—æ®µ
        å½“ include_train_data=true æ—¶ï¼Œåç«¯ç›´æ¥è¯»å– CSV å¹¶è§£æä¸ºç»“æ„åŒ–æ•°æ®è¿”å›
        """
        from apps.core.logger import opspilot_logger as logger
        import pandas as pd
        
        representation = super().to_representation(instance)
        
        # å¤„ç† train_dataï¼šåç«¯ç›´æ¥è¯»å–å¹¶è§£æ CSV
        if self.include_train_data and instance.train_data:
            try:
                # è¯»å– CSV æ–‡ä»¶
                df = pd.read_csv(instance.train_data.open('rb'))
                
                # ğŸ”¥ å¤„ç† timestamp å­—æ®µï¼šè½¬æ¢ä¸º Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                if 'timestamp' in df.columns:
                    try:
                        # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
                        df['timestamp'] = pd.to_datetime(df['timestamp'])
                        # è½¬æ¢ä¸º Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                        df['timestamp'] = (df['timestamp'].astype('int64') / 1e9).astype('int64')
                    except Exception as e:
                        logger.warning(f"Failed to parse timestamp column: {e}")
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä¿æŒåŸå€¼
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨å¹¶æ·»åŠ ç´¢å¼•
                data_list = df.to_dict('records')
                for i, row in enumerate(data_list):
                    row['index'] = i
                
                representation['train_data'] = data_list
                logger.info(f"Successfully loaded train_data for instance {instance.id}: {len(data_list)} rows")
                
            except Exception as e:
                logger.error(f"Failed to read train_data for instance {instance.id}: {e}", exc_info=True)
                representation['train_data'] = []
                representation['error'] = f"è¯»å–è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}"
        elif not self.include_train_data:
            representation.pop("train_data", None)
        
        # å¤„ç† metadataï¼šS3JSONField è‡ªåŠ¨å¤„ç†ï¼Œç›´æ¥è¿”å›å¯¹è±¡
        if self.include_metadata and instance.metadata:
            # S3JSONField ä¼šè‡ªåŠ¨ä» MinIO è¯»å–å¹¶è§£å‹
            representation['metadata'] = instance.metadata
        elif not self.include_metadata:
            representation.pop("metadata", None)
        
        return representation


class AnomalyDetectionDatasetReleaseSerializer(AuthSerializer):
    """å¼‚å¸¸æ£€æµ‹æ•°æ®é›†å‘å¸ƒç‰ˆæœ¬åºåˆ—åŒ–å™¨"""
    permission_key = "dataset.anomaly_detection_dataset_release"
    
    # æ·»åŠ åªå†™å­—æ®µç”¨äºæ¥æ”¶æ–‡ä»¶ID
    train_file_id = serializers.IntegerField(write_only=True, required=False)
    val_file_id = serializers.IntegerField(write_only=True, required=False)
    test_file_id = serializers.IntegerField(write_only=True, required=False)

    class Meta:
        model = AnomalyDetectionDatasetRelease
        fields = "__all__"
        extra_kwargs = {
            'name': {'required': False},  # åˆ›å»ºæ—¶å¯é€‰ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆ
            'dataset_file': {'required': False},  # åˆ›å»ºæ—¶ä¸éœ€è¦ç›´æ¥æä¾›æ–‡ä»¶
            'file_size': {'required': False},
            'status': {'required': False},
        }
    
    def create(self, validated_data):
        """
        è‡ªå®šä¹‰åˆ›å»ºæ–¹æ³•ï¼Œæ”¯æŒä»æ–‡ä»¶IDåˆ›å»ºæ•°æ®é›†å‘å¸ƒç‰ˆæœ¬
        """
        from apps.core.logger import opspilot_logger as logger
        
        # æå–æ–‡ä»¶ID
        train_file_id = validated_data.pop('train_file_id', None)
        val_file_id = validated_data.pop('val_file_id', None)
        test_file_id = validated_data.pop('test_file_id', None)
        
        # å¦‚æœæä¾›äº†æ–‡ä»¶IDï¼Œåˆ™æ‰§è¡Œæ–‡ä»¶æ‰“åŒ…é€»è¾‘
        if train_file_id and val_file_id and test_file_id:
            return self._create_from_files(validated_data, train_file_id, val_file_id, test_file_id)
        else:
            # å¦åˆ™ä½¿ç”¨æ ‡å‡†åˆ›å»ºï¼ˆé€‚ç”¨äºç›´æ¥ä¸Šä¼ ZIPæ–‡ä»¶çš„åœºæ™¯ï¼‰
            return super().create(validated_data)
    
    def _create_from_files(self, validated_data, train_file_id, val_file_id, test_file_id):
        """
        ä»è®­ç»ƒæ•°æ®æ–‡ä»¶IDåˆ›å»ºæ•°æ®é›†å‘å¸ƒç‰ˆæœ¬ï¼ˆå¼‚æ­¥ï¼‰
        
        åˆ›å»º pending çŠ¶æ€çš„è®°å½•ï¼Œè§¦å‘ Celery ä»»åŠ¡è¿›è¡Œå¼‚æ­¥å¤„ç†
        """
        from apps.core.logger import opspilot_logger as logger
        
        dataset = validated_data.get('dataset')
        version = validated_data.get('version')
        name = validated_data.get('name')
        description = validated_data.get('description', '')
        
        try:
            # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            train_obj = AnomalyDetectionTrainData.objects.get(id=train_file_id, dataset=dataset)
            val_obj = AnomalyDetectionTrainData.objects.get(id=val_file_id, dataset=dataset)
            test_obj = AnomalyDetectionTrainData.objects.get(id=test_file_id, dataset=dataset)
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒç‰ˆæœ¬çš„è®°å½•ï¼ˆå¹‚ç­‰æ€§ä¿æŠ¤ï¼‰
            existing = AnomalyDetectionDatasetRelease.objects.filter(
                dataset=dataset,
                version=version
            ).exclude(status='failed').first()
            
            if existing:
                logger.info(f"æ•°æ®é›†ç‰ˆæœ¬å·²å­˜åœ¨ - Dataset: {dataset.id}, Version: {version}, Status: {existing.status}")
                return existing
            
            # åˆ›å»º pending çŠ¶æ€çš„å‘å¸ƒè®°å½•
            validated_data['status'] = 'pending'
            validated_data['file_size'] = 0
            validated_data['metadata'] = {}
            
            if not name:
                validated_data['name'] = f"{dataset.name}_v{version}"
            
            if not description:
                validated_data['description'] = f"ä»æ•°æ®é›†æ–‡ä»¶æ‰‹åŠ¨å‘å¸ƒ: {train_obj.name}, {val_obj.name}, {test_obj.name}"
            
            release = AnomalyDetectionDatasetRelease.objects.create(**validated_data)
            
            # è§¦å‘å¼‚æ­¥ä»»åŠ¡
            from apps.mlops.tasks.anomaly_detection import publish_dataset_release_async
            publish_dataset_release_async.delay(
                release.id,
                train_file_id,
                val_file_id,
                test_file_id
            )
            
            logger.info(f"åˆ›å»ºæ•°æ®é›†å‘å¸ƒä»»åŠ¡ - Release ID: {release.id}, Dataset: {dataset.id}, Version: {version}")
            
            return release
            
        except AnomalyDetectionTrainData.DoesNotExist as e:
            logger.error(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {str(e)}")
            raise serializers.ValidationError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸å±äºè¯¥æ•°æ®é›†")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®é›†å‘å¸ƒä»»åŠ¡å¤±è´¥ - {str(e)}", exc_info=True)
            raise serializers.ValidationError(f"åˆ›å»ºå‘å¸ƒä»»åŠ¡å¤±è´¥: {str(e)}")


class AnomalyDetectionTrainJobSerializer(AuthSerializer):
    permission_key = "dataset.anomaly_detection_train_job"

    class Meta:
        model = AnomalyDetectionTrainJob
        fields = "__all__"
        extra_kwargs = {
            'config_url': {
                'write_only': True,  # å‰ç«¯ä¸éœ€è¦çœ‹åˆ° MinIO è·¯å¾„
                'required': False
            }
        }
    
    def validate(self, attrs):
        """
        éªŒè¯åˆ›å»ºæ—¶ dataset_version å¿…é¡»ä¼ å…¥
        """
        # åªåœ¨åˆ›å»ºæ—¶éªŒè¯ï¼ˆæ›´æ–°æ—¶ä¸å¼ºåˆ¶è¦æ±‚ï¼‰
        if not self.instance and not attrs.get('dataset_version'):
            raise serializers.ValidationError({
                'dataset_version': 'åˆ›å»ºè®­ç»ƒä»»åŠ¡æ—¶å¿…é¡»æŒ‡å®šæ•°æ®é›†ç‰ˆæœ¬'
            })
        return super().validate(attrs)


class TimeSeriesDataSerializer(serializers.Serializer):
    """æ—¶åºæ•°æ®ç‚¹åºåˆ—åŒ–å™¨"""
    timestamp = serializers.CharField(help_text="æ—¶é—´æˆ³ï¼Œæ ¼å¼: YYYY-MM-DD HH:MM:SS")
    value = serializers.FloatField(help_text="æ•°å€¼")
    label = serializers.IntegerField(required=False, help_text="æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰")


class AnomalyDetectionPredictRequestSerializer(serializers.Serializer):
    """å¼‚å¸¸æ£€æµ‹é¢„æµ‹è¯·æ±‚åºåˆ—åŒ–å™¨"""
    model_name = serializers.CharField(
        max_length=100,
        help_text="æ¨¡å‹åç§°"
    )
    model_version = serializers.CharField(
        max_length=50,
        default="latest",
        help_text="æ¨¡å‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸ºlatest"
    )
    algorithm = serializers.ChoiceField(
        choices=[('RandomForest', 'RandomForest')],
        help_text="ç®—æ³•ç±»å‹"
    )
    data = TimeSeriesDataSerializer(
        many=True,
        help_text="æ—¶åºæ•°æ®åˆ—è¡¨"
    )
    anomaly_threshold = serializers.FloatField(
        default=0.5,
        min_value=0.0,
        max_value=1.0,
        help_text="å¼‚å¸¸åˆ¤å®šé˜ˆå€¼ï¼ŒèŒƒå›´[0,1]"
    )

    def validate_data(self, value):
        """éªŒè¯æ—¶åºæ•°æ®"""
        if not value:
            raise serializers.ValidationError("æ•°æ®ä¸èƒ½ä¸ºç©º")
        if len(value) < 2:
            raise serializers.ValidationError("è‡³å°‘éœ€è¦2ä¸ªæ•°æ®ç‚¹")
        return value


class PredictionResultSerializer(serializers.Serializer):
    """å•ä¸ªé¢„æµ‹ç»“æœåºåˆ—åŒ–å™¨"""
    timestamp = serializers.CharField(help_text="æ—¶é—´æˆ³")
    value = serializers.FloatField(help_text="åŸå§‹æ•°å€¼")
    anomaly_probability = serializers.FloatField(help_text="å¼‚å¸¸æ¦‚ç‡")
    is_anomaly = serializers.IntegerField(help_text="æ˜¯å¦å¼‚å¸¸ï¼Œ1ä¸ºå¼‚å¸¸ï¼Œ0ä¸ºæ­£å¸¸")


class AnomalyDetectionPredictResponseSerializer(serializers.Serializer):
    """å¼‚å¸¸æ£€æµ‹é¢„æµ‹å“åº”åºåˆ—åŒ–å™¨"""
    success = serializers.BooleanField(help_text="é¢„æµ‹æ˜¯å¦æˆåŠŸ")
    model_name = serializers.CharField(help_text="ä½¿ç”¨çš„æ¨¡å‹åç§°")
    model_version = serializers.CharField(help_text="ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬")
    algorithm = serializers.CharField(help_text="ä½¿ç”¨çš„ç®—æ³•")
    anomaly_threshold = serializers.FloatField(help_text="å¼‚å¸¸åˆ¤å®šé˜ˆå€¼")
    total_points = serializers.IntegerField(help_text="æ€»æ•°æ®ç‚¹æ•°")
    anomaly_count = serializers.IntegerField(help_text="å¼‚å¸¸ç‚¹æ•°é‡")
    predictions = PredictionResultSerializer(
        many=True,
        help_text="é¢„æµ‹ç»“æœåˆ—è¡¨"
    )


class AnomalyDetectionServingSerializer(AuthSerializer):

    permission_key = "serving.anomaly_detection_serving"

    class Meta:
        model = AnomalyDetectionServing
        fields = "__all__"
